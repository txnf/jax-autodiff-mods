{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Primitive(NamedTuple):\n",
    "  name: str\n",
    "\n",
    "add_p = Primitive('add')\n",
    "mul_p = Primitive('mul')\n",
    "neg_p = Primitive(\"neg\")\n",
    "sin_p = Primitive(\"sin\")\n",
    "cos_p = Primitive(\"cos\")\n",
    "\n",
    "exp_p = Primitive(\"exp\")\n",
    "\n",
    "reduce_sum_p = Primitive(\"reduce_sum\")\n",
    "greater_p = Primitive(\"greater\")\n",
    "less_p = Primitive(\"less\")\n",
    "transpose_p = Primitive(\"transpose\")\n",
    "broadcast_p = Primitive(\"broadcast\")\n",
    "\n",
    "def add(x, y): return bind1(add_p, x, y)\n",
    "def mul(x, y): return bind1(mul_p, x, y)\n",
    "def neg(x): return bind1(neg_p, x)\n",
    "def sin(x): return bind1(sin_p, x)\n",
    "def cos(x): return bind1(cos_p, x)\n",
    "\n",
    "def exp(x): return bind1(exp_p, x)\n",
    "\n",
    "def greater(x, y): return bind1(greater_p, x, y)\n",
    "def less(x, y): return bind1(less_p, x, y)\n",
    "def transpose(x, perm): return bind1(transpose_p, x, perm=perm)\n",
    "def broadcast(x, shape, axes): return bind1(broadcast_p, x, shape=shape, axes=axes)\n",
    "def reduce_sum(x, axis=None):\n",
    "  if axis is None:\n",
    "    axis = tuple(range(np.ndim(x)))\n",
    "  if type(axis) is int:\n",
    "    axis = (axis,)\n",
    "  return bind1(reduce_sum_p, x, axis=axis)\n",
    "\n",
    "def bind1(prim, *args, **params):\n",
    "  out, = bind(prim, *args, **params)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, Any\n",
    "\n",
    "class MainTrace(NamedTuple):\n",
    "  level: int\n",
    "  trace_type: type['Trace']\n",
    "  global_data: Optional[Any]\n",
    "\n",
    "trace_stack: list[MainTrace] = []\n",
    "dynamic_trace: Optional[MainTrace] = None  # to be employed in Part 3\n",
    "\n",
    "@contextmanager\n",
    "def new_main(trace_type: type['Trace'], global_data=None):\n",
    "  level = len(trace_stack)\n",
    "  main = MainTrace(level, trace_type, global_data)\n",
    "  trace_stack.append(main)\n",
    "\n",
    "  try:\n",
    "    yield main\n",
    "  finally:\n",
    "    trace_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace:\n",
    "  main: MainTrace\n",
    "\n",
    "  def __init__(self, main: MainTrace) -> None:\n",
    "    self.main = main\n",
    "\n",
    "  def pure(self, val): assert False  # must override\n",
    "  def lift(self, val): assert False  # must override\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    assert False  # must override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tracer:\n",
    "  _trace: Trace\n",
    "\n",
    "  __array_priority__ = 1000\n",
    "\n",
    "  @property\n",
    "  def aval(self):\n",
    "    assert False  # must override\n",
    "\n",
    "  def full_lower(self):\n",
    "    return self  # default implementation\n",
    "\n",
    "  def __neg__(self): return self.aval._neg(self)\n",
    "  def __add__(self, other): return self.aval._add(self, other)\n",
    "  def __radd__(self, other): return self.aval._radd(self, other)\n",
    "  def __mul__(self, other): return self.aval._mul(self, other)\n",
    "  def __rmul__(self, other): return self.aval._rmul(self, other)\n",
    "  def __gt__(self, other): return self.aval._gt(self, other)\n",
    "  def __lt__(self, other): return self.aval._lt(self, other)\n",
    "  def __bool__(self): return self.aval._bool(self)\n",
    "  def __nonzero__(self): return self.aval._nonzero(self)\n",
    "\n",
    "  def __getattr__(self, name):\n",
    "    try:\n",
    "      return getattr(self.aval, name)\n",
    "    except AttributeError:\n",
    "      raise AttributeError(f\"{self.__class__.__name__} has no attribute {name}\")\n",
    "\n",
    "def swap(f): return lambda x, y: f(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapedArray:\n",
    "  array_abstraction_level = 1\n",
    "  shape: tuple[int, ...]\n",
    "  dtype: np.dtype\n",
    "\n",
    "  def __init__(self, shape, dtype):\n",
    "    self.shape = shape\n",
    "    self.dtype = dtype\n",
    "\n",
    "  @property\n",
    "  def ndim(self):\n",
    "    return len(self.shape)\n",
    "\n",
    "  _neg = staticmethod(neg)\n",
    "  _add = staticmethod(add)\n",
    "  _radd = staticmethod(swap(add))\n",
    "  _mul = staticmethod(mul)\n",
    "  _rmul = staticmethod(swap(mul))\n",
    "  _gt = staticmethod(greater)\n",
    "  _lt = staticmethod(less)\n",
    "\n",
    "  @staticmethod\n",
    "  def _bool(tracer):\n",
    "    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n",
    "\n",
    "  @staticmethod\n",
    "  def _nonzero(tracer):\n",
    "    raise Exception(\"ShapedArray can't be unambiguously converted to bool\")\n",
    "\n",
    "  def str_short(self):\n",
    "    return f'{self.dtype.name}[{\",\".join(str(d) for d in self.shape)}]'\n",
    "\n",
    "  def __hash__(self):\n",
    "    return hash((self.shape, self.dtype))\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return (type(self) is type(other) and\n",
    "            self.shape == other.shape and self.dtype == other.dtype)\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"ShapedArray(shape={self.shape}, dtype={self.dtype})\"\n",
    "\n",
    "class ConcreteArray(ShapedArray):\n",
    "  array_abstraction_level = 2\n",
    "  val: np.ndarray\n",
    "\n",
    "  def __init__(self, val):\n",
    "    self.val = val\n",
    "    self.shape = val.shape\n",
    "    self.dtype = val.dtype\n",
    "\n",
    "  @staticmethod\n",
    "  def _bool(tracer):\n",
    "    return bool(tracer.aval.val)\n",
    "\n",
    "  @staticmethod\n",
    "  def _nonzero(tracer):\n",
    "    return bool(tracer.aval.val)\n",
    "\n",
    "def get_aval(x):\n",
    "  if isinstance(x, Tracer):\n",
    "    return x.aval\n",
    "  elif type(x) in jax_types:\n",
    "    return ConcreteArray(np.asarray(x))\n",
    "  else:\n",
    "    raise TypeError(x)\n",
    "\n",
    "jax_types = {bool, int, float,\n",
    "             np.bool_, np.int32, np.int64, np.float32, np.float64, np.ndarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(prim, *args, **params):\n",
    "  top_trace = find_top_trace(args)\n",
    "  tracers = [full_raise(top_trace, arg) for arg in args]\n",
    "  outs = top_trace.process_primitive(prim, tracers, params)\n",
    "  return [full_lower(out) for out in outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op\n",
    "\n",
    "def find_top_trace(xs) -> Trace:\n",
    "  top_main = max((x._trace.main for x in xs if isinstance(x, Tracer)),\n",
    "                 default=trace_stack[0], key=op.attrgetter('level'))\n",
    "  if dynamic_trace and dynamic_trace.level > top_main.level:\n",
    "    top_main = dynamic_trace\n",
    "  return top_main.trace_type(top_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_lower(val: Any):\n",
    "  if isinstance(val, Tracer):\n",
    "    return val.full_lower()\n",
    "  else:\n",
    "    return val\n",
    "\n",
    "def full_raise(trace: Trace, val: Any) -> Tracer:\n",
    "  if not isinstance(val, Tracer):\n",
    "    assert type(val) in jax_types\n",
    "    return trace.pure(val)\n",
    "  level = trace.main.level\n",
    "  if val._trace.main is trace.main:\n",
    "    return val\n",
    "  elif val._trace.main.level < level:\n",
    "    return trace.lift(val)\n",
    "  elif val._trace.main.level > level:\n",
    "    raise Exception(f\"Can't lift level {val._trace.main.level} to {level}.\")\n",
    "  else:  # val._trace.level == level\n",
    "    raise Exception(f\"Different traces at same level: {val._trace}, {trace}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalTrace(Trace):\n",
    "  pure = lift = lambda self, x: x  # no boxing in Tracers needed\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    return impl_rules[primitive](*tracers, **params)\n",
    "\n",
    "trace_stack.append(MainTrace(0, EvalTrace, None))  # special bottom of the stack\n",
    "\n",
    "# NB: in JAX, instead of a dict we attach impl rules to the Primitive instance\n",
    "impl_rules = {}\n",
    "\n",
    "impl_rules[add_p] = lambda x, y: [np.add(x, y)]\n",
    "impl_rules[mul_p] = lambda x, y: [np.multiply(x, y)]\n",
    "impl_rules[neg_p] = lambda x: [np.negative(x)]\n",
    "impl_rules[sin_p] = lambda x: [np.sin(x)]\n",
    "impl_rules[cos_p] = lambda x: [np.cos(x)]\n",
    "\n",
    "impl_rules[exp_p] = lambda x: [np.exp(x)]\n",
    "\n",
    "impl_rules[reduce_sum_p] = lambda x, *, axis: [np.sum(x, axis)]\n",
    "impl_rules[greater_p] = lambda x, y: [np.greater(x, y)]\n",
    "impl_rules[less_p] = lambda x, y: [np.less(x, y)]\n",
    "impl_rules[transpose_p] = lambda x, *, perm: [np.transpose(x, perm)]\n",
    "\n",
    "def broadcast_impl(x, *, shape, axes):\n",
    "  for axis in sorted(axes):\n",
    "    x = np.expand_dims(x, axis)\n",
    "  return [np.broadcast_to(x, shape)]\n",
    "impl_rules[broadcast_p] = broadcast_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7177599838802657\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  y = sin(x) * 2.\n",
    "  z = - y + x\n",
    "  return z\n",
    "\n",
    "print(f(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORWARD MODE AUTODIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "\n",
    "def zeros_like(val):\n",
    "  aval = get_aval(val)\n",
    "  return np.zeros(aval.shape, aval.dtype)\n",
    "\n",
    "def unzip2(pairs):\n",
    "  lst1, lst2 = [], []\n",
    "  for x1, x2 in pairs:\n",
    "    lst1.append(x1)\n",
    "    lst2.append(x2)\n",
    "  return lst1, lst2\n",
    "\n",
    "def map(f, *xs):\n",
    "  return list(builtins.map(f, *xs))\n",
    "\n",
    "def zip(*args):\n",
    "  fst, *rest = args = map(list, args)\n",
    "  n = len(fst)\n",
    "  for arg in rest:\n",
    "    assert len(arg) == n\n",
    "  return list(builtins.zip(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JVPTracer(Tracer):\n",
    "  def __init__(self, trace, primal, tangent):\n",
    "    self._trace = trace\n",
    "    self.primal = primal\n",
    "    self.tangent = tangent\n",
    "\n",
    "  @property\n",
    "  def aval(self):\n",
    "    return get_aval(self.primal)\n",
    "\n",
    "class JVPTrace(Trace):\n",
    "  pure = lift = lambda self, val: JVPTracer(self, val, zeros_like(val))\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    primals_in, tangents_in = unzip2((t.primal, t.tangent) for t in tracers)\n",
    "    jvp_rule = jvp_rules[primitive]\n",
    "    primal_outs, tangent_outs = jvp_rule(primals_in, tangents_in, **params)\n",
    "    return [JVPTracer(self, x, t) for x, t in zip(primal_outs, tangent_outs)]\n",
    "\n",
    "jvp_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SRJVPTracer(Tracer):\n",
    "  def __init__(self, trace, primal, tangent):\n",
    "    self._trace = trace\n",
    "    self.primal = primal\n",
    "    self.tangent = tangent\n",
    "    self.sr_tangent = tangent\n",
    "\n",
    "  @property\n",
    "  def aval(self):\n",
    "    return get_aval(self.primal)\n",
    "\n",
    "class SRJVPTrace(Trace):\n",
    "  pure = lift = lambda self, val: JVPTracer(self, val, zeros_like(val))\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    primals_in, tangents_in = unzip2((t.primal, t.tangent) for t in tracers)\n",
    "    srjvp_rule = jvp_rules[primitive]\n",
    "    primal_outs, tangent_outs = jvp_rule(primals_in, tangents_in, **params)\n",
    "    return [JVPTracer(self, x, t) for x, t in zip(primal_outs, tangent_outs)]\n",
    "\n",
    "srjvp_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function max>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.0], [1.0])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semiring mod to ops\n",
    "import functools as ft\n",
    "import operator as op\n",
    "\n",
    "mx = lambda a,b: max(a,b)\n",
    "mn = lambda a,b: min(a,b)\n",
    "\n",
    "def with_srops(sr_add=op.__add__, sr_mul=op.__mul__):\n",
    "  def wrapper(f):\n",
    "    @ft.wraps(f)\n",
    "    def rv(a,b):\n",
    "      return f(sr_add, sr_mul, a, b)\n",
    "    return rv\n",
    "  return wrapper\n",
    "\n",
    "@with_srops(mx, op.__mul__)\n",
    "def add_jvp(sr_add, sr_mul, primals, tangents):\n",
    "  (x, y), (x_dot, y_dot) = primals, tangents\n",
    "#   return [x + y], [x_dot + y_dot]\n",
    "  return [x + y], [sr_add(x_dot, y_dot)]\n",
    "\n",
    "@with_srops\n",
    "def mul_jvp(sr_add, sr_mul, primals, tangents):\n",
    "  (x, y), (x_dot, y_dot) = primals, tangents\n",
    "#   return [x * y], [x_dot * y + x * y_dot]\n",
    "  return [x * y], [sr_add(sr_mul(x_dot, y), sr_mul(x, y_dot))]\n",
    "\n",
    "# add_jvp(op.__add__, op.__mul__, (1.0, 1.0), (1.0, 1.0))\n",
    "\n",
    "add_jvp((1.0, 1.0), (1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jvp(primals, tangents):\n",
    "  (x, y), (x_dot, y_dot) = primals, tangents\n",
    "  return [x + y], [x_dot + y_dot]\n",
    "jvp_rules[add_p] = add_jvp\n",
    "\n",
    "def mul_jvp(primals, tangents):\n",
    "  (x, y), (x_dot, y_dot) = primals, tangents\n",
    "  return [x * y], [x_dot * y + x * y_dot]\n",
    "jvp_rules[mul_p] = mul_jvp\n",
    "\n",
    "def sin_jvp(primals, tangents):\n",
    "  (x,), (x_dot,) = primals, tangents\n",
    "  return [sin(x)], [cos(x) * x_dot]\n",
    "jvp_rules[sin_p] = sin_jvp\n",
    "\n",
    "def cos_jvp(primals, tangents):\n",
    "  (x,), (x_dot,) = primals, tangents\n",
    "  return [cos(x)], [-sin(x) * x_dot]\n",
    "jvp_rules[cos_p] = cos_jvp\n",
    "\n",
    "def neg_jvp(primals, tangents):\n",
    "  (x,), (x_dot,) = primals, tangents\n",
    "  return [neg(x)], [neg(x_dot)]\n",
    "jvp_rules[neg_p] = neg_jvp\n",
    "\n",
    "\n",
    "def exp_jvp(primals, tangents):\n",
    "  (x,), (x_dot,) = primals, tangents\n",
    "  return [exp(x)], [exp(x) * x_dot]\n",
    "jvp_rules[exp_p] = exp_jvp\n",
    "\n",
    "\n",
    "def reduce_sum_jvp(primals, tangents, *, axis):\n",
    "  (x,), (x_dot,) = primals, tangents\n",
    "  return [reduce_sum(x, axis)], [reduce_sum(x_dot, axis)]\n",
    "jvp_rules[reduce_sum_p] = reduce_sum_jvp\n",
    "\n",
    "def greater_jvp(primals, tangents):\n",
    "  (x, y), _ = primals, tangents\n",
    "  out_primal = greater(x, y)\n",
    "  return [out_primal], [zeros_like(out_primal)]\n",
    "jvp_rules[greater_p] = greater_jvp\n",
    "\n",
    "def less_jvp(primals, tangents):\n",
    "  (x, y), _ = primals, tangents\n",
    "  out_primal = less(x, y)\n",
    "  return [out_primal], [zeros_like(out_primal)]\n",
    "jvp_rules[less_p] = less_jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jvp_v1(f, primals, tangents):\n",
    "  with new_main(JVPTrace) as main:\n",
    "    trace = JVPTrace(main)\n",
    "    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n",
    "    out = f(*tracers_in)\n",
    "    tracer_out = full_raise(trace, out)\n",
    "    primal_out, tangent_out = tracer_out.primal, tracer_out.tangent\n",
    "  return primal_out, tangent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jvp_flat(f, primals, tangents):\n",
    "  with new_main(JVPTrace) as main:\n",
    "    trace = JVPTrace(main)\n",
    "    tracers_in = [JVPTracer(trace, x, t) for x, t in zip(primals, tangents)]\n",
    "    outs = f(*tracers_in)\n",
    "    tracers_out = [full_raise(trace, out) for out in outs]\n",
    "    primals_out, tangents_out = unzip2((t.primal, t.tangent) for t in tracers_out)\n",
    "  return primals_out, tangents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jvp(f, primals, tangents):\n",
    "  primals_flat, in_tree = tree_flatten(primals)\n",
    "  tangents_flat, in_tree2 = tree_flatten(tangents)\n",
    "  if in_tree != in_tree2: raise TypeError\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "  primals_out_flat, tangents_out_flat = jvp_flat(f, primals_flat, tangents_flat)\n",
    "  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n",
    "  tangents_out = tree_unflatten(out_tree(), tangents_out_flat)\n",
    "  return primals_out, tangents_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod JVP to incorporate semiring operations|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf_jvp\u001b[39m(x, x_dot):\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m jvp(f, (x,), (x_dot,))\n\u001b[0;32m----> 7\u001b[0m make_jaxpr(f_jvp, \u001b[39m1.0\u001b[39;49m, \u001b[39m1.0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m, in \u001b[0;36mmake_jaxpr\u001b[0;34m(f, *avals_in)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m new_dynamic(main):\n\u001b[1;32m     19\u001b[0m   trace \u001b[39m=\u001b[39m JaxprTrace(main)\n\u001b[0;32m---> 20\u001b[0m   tracers_in \u001b[39m=\u001b[39m [trace\u001b[39m.\u001b[39;49mnew_arg(aval) \u001b[39mfor\u001b[39;49;00m aval \u001b[39min\u001b[39;49;00m avals_in]\n\u001b[1;32m     21\u001b[0m   outs \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39mtracers_in)\n\u001b[1;32m     22\u001b[0m   tracers_out \u001b[39m=\u001b[39m [full_raise(trace, out) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m new_dynamic(main):\n\u001b[1;32m     19\u001b[0m   trace \u001b[39m=\u001b[39m JaxprTrace(main)\n\u001b[0;32m---> 20\u001b[0m   tracers_in \u001b[39m=\u001b[39m [trace\u001b[39m.\u001b[39;49mnew_arg(aval) \u001b[39mfor\u001b[39;00m aval \u001b[39min\u001b[39;00m avals_in]\n\u001b[1;32m     21\u001b[0m   outs \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39mtracers_in)\n\u001b[1;32m     22\u001b[0m   tracers_out \u001b[39m=\u001b[39m [full_raise(trace, out) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m, in \u001b[0;36mJaxprTrace.new_arg\u001b[0;34m(self, aval)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_arg\u001b[39m(\u001b[39mself\u001b[39m, aval: ShapedArray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JaxprTracer:\n\u001b[0;32m---> 13\u001b[0m   aval \u001b[39m=\u001b[39m raise_to_shaped(aval)\n\u001b[1;32m     14\u001b[0m   tracer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mnew_tracer(\u001b[39mself\u001b[39m, aval)\n\u001b[1;32m     15\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mtracer_to_var[\u001b[39mid\u001b[39m(tracer)] \u001b[39m=\u001b[39m Var(aval)\n",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m, in \u001b[0;36mraise_to_shaped\u001b[0;34m(aval)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_to_shaped\u001b[39m(aval):\n\u001b[0;32m---> 30\u001b[0m   \u001b[39mreturn\u001b[39;00m ShapedArray(aval\u001b[39m.\u001b[39;49mshape, aval\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return sin(x) + x*2\n",
    "\n",
    "def f_jvp(x, x_dot):\n",
    "    return jvp(f, (x,), (x_dot,))\n",
    "\n",
    "make_jaxpr(f_jvp, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<functools._lru_cache_wrapper at 0x7f2e1b1ecca0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_fun(f, in_tree):\n",
    "  store = Store()\n",
    "\n",
    "  def flat_fun(*args_flat):\n",
    "    pytree_args = tree_unflatten(in_tree, args_flat)\n",
    "    out = f(*pytree_args)\n",
    "    out_flat, out_tree = tree_flatten(out)\n",
    "    store.set_value(out_tree)\n",
    "    return out_flat\n",
    "\n",
    "  return flat_fun, store\n",
    "\n",
    "class Empty: pass\n",
    "empty = Empty()\n",
    "\n",
    "class Store:\n",
    "  val = empty\n",
    "\n",
    "  def set_value(self, val):\n",
    "    assert self.val is empty\n",
    "    self.val = val\n",
    "\n",
    "  def __call__(self):\n",
    "    return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Hashable, Iterable, Iterator\n",
    "import itertools as it\n",
    "from typing import Callable\n",
    "\n",
    "class NodeType(NamedTuple):\n",
    "  name: str\n",
    "  to_iterable: Callable\n",
    "  from_iterable: Callable\n",
    "\n",
    "def register_pytree_node(ty: type, to_iter: Callable, from_iter: Callable\n",
    "                         ) -> None:\n",
    "  node_types[ty] = NodeType(str(ty), to_iter, from_iter)\n",
    "\n",
    "node_types: dict[type, NodeType] = {}\n",
    "register_pytree_node(tuple, lambda t: (None, t), lambda _, xs: tuple(xs))\n",
    "register_pytree_node(list,  lambda l: (None, l), lambda _, xs:  list(xs))\n",
    "register_pytree_node(dict,\n",
    "                     lambda d: map(tuple, unzip2(sorted(d.items()))),\n",
    "                     lambda keys, vals: dict(zip(keys, vals)))\n",
    "\n",
    "class PyTreeDef(NamedTuple):\n",
    "  node_type: NodeType\n",
    "  node_metadata: Hashable\n",
    "  child_treedefs: tuple['PyTreeDef', ...]\n",
    "\n",
    "class Leaf: pass\n",
    "leaf = Leaf()\n",
    "\n",
    "def tree_flatten(x: Any) -> tuple[list[Any], PyTreeDef]:\n",
    "  children_iter, treedef = _tree_flatten(x)\n",
    "  return list(children_iter), treedef\n",
    "\n",
    "def _tree_flatten(x: Any) -> tuple[Iterable, PyTreeDef]:\n",
    "  node_type = node_types.get(type(x))\n",
    "  if node_type:\n",
    "    node_metadata, children = node_type.to_iterable(x)\n",
    "    children_flat, child_trees = unzip2(map(_tree_flatten, children))\n",
    "    flattened = it.chain.from_iterable(children_flat)\n",
    "    return flattened, PyTreeDef(node_type, node_metadata, tuple(child_trees))\n",
    "  else:\n",
    "    return [x], leaf\n",
    "\n",
    "def tree_unflatten(treedef: PyTreeDef, xs: list[Any]) -> Any:\n",
    "  return _tree_unflatten(treedef, iter(xs))\n",
    "\n",
    "def _tree_unflatten(treedef: PyTreeDef, xs: Iterator) -> Any:\n",
    "  if treedef is leaf:\n",
    "    return next(xs)\n",
    "  else:\n",
    "    children = (_tree_unflatten(t, xs) for t in treedef.child_treedefs)\n",
    "    return treedef.node_type.from_iterable(treedef.node_metadata, children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped_aval(batch_dim, aval):\n",
    "  shape = list(aval.shape)\n",
    "  del shape[batch_dim]\n",
    "  return ShapedArray(tuple(shape), aval.dtype)\n",
    "\n",
    "def move_batch_axis(axis_size, src, dst, x):\n",
    "  if src is not_mapped:\n",
    "    target_shape = list(np.shape(x))\n",
    "    target_shape.insert(dst, axis_size)\n",
    "    return broadcast(x, target_shape, [dst])\n",
    "  elif src == dst:\n",
    "    return x\n",
    "  else:\n",
    "    return moveaxis(x, src, dst)\n",
    "\n",
    "def moveaxis(x, src: int, dst: int):\n",
    "  perm = [i for i in range(np.ndim(x)) if i != src]\n",
    "  perm.insert(dst, src)\n",
    "  return transpose(x, perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class NotMapped: pass\n",
    "not_mapped = NotMapped()\n",
    "\n",
    "BatchAxis = Union[NotMapped, int]\n",
    "\n",
    "class BatchTracer(Tracer):\n",
    "  def __init__(self, trace, val, batch_dim: BatchAxis):\n",
    "    self._trace = trace\n",
    "    self.val = val\n",
    "    self.batch_dim = batch_dim\n",
    "\n",
    "  @property\n",
    "  def aval(self):\n",
    "    if self.batch_dim is not_mapped:\n",
    "      return get_aval(self.val)\n",
    "    else:\n",
    "      return mapped_aval(self.batch_dim, get_aval(self.val))\n",
    "\n",
    "  def full_lower(self):\n",
    "    if self.batch_dim is not_mapped:\n",
    "      return full_lower(self.val)\n",
    "    else:\n",
    "      return self\n",
    "\n",
    "class BatchTrace(Trace):\n",
    "  pure = lift = lambda self, val: BatchTracer(self, val, not_mapped)\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    vals_in, bdims_in = unzip2((t.val, t.batch_dim) for t in tracers)\n",
    "    vmap_rule = vmap_rules[primitive]\n",
    "    val_outs, bdim_outs = vmap_rule(self.axis_size, vals_in, bdims_in, **params)\n",
    "    return [BatchTracer(self, x, bd) for x, bd in zip(val_outs, bdim_outs)]\n",
    "\n",
    "  @property\n",
    "  def axis_size(self):\n",
    "    return self.main.global_data\n",
    "\n",
    "vmap_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def binop_batching_rule(op, axis_size, vals_in, dims_in):\n",
    "  (x, y), (x_bdim, y_bdim) = vals_in, dims_in\n",
    "  if x_bdim != y_bdim:\n",
    "    if x_bdim is not_mapped:\n",
    "      x = move_batch_axis(axis_size, x_bdim, y_bdim, x)\n",
    "      x_bdim = y_bdim\n",
    "    else:\n",
    "      y = move_batch_axis(axis_size, y_bdim, x_bdim, y)\n",
    "  return [op(x, y)], [x_bdim]\n",
    "vmap_rules[add_p] = partial(binop_batching_rule, add)\n",
    "vmap_rules[mul_p] = partial(binop_batching_rule, mul)\n",
    "\n",
    "def vectorized_unop_batching_rule(op, axis_size, vals_in, dims_in):\n",
    "  (x,), (x_bdim,) = vals_in, dims_in\n",
    "  return [op(x)], [x_bdim]\n",
    "vmap_rules[sin_p] = partial(vectorized_unop_batching_rule, sin)\n",
    "vmap_rules[cos_p] = partial(vectorized_unop_batching_rule, cos)\n",
    "vmap_rules[neg_p] = partial(vectorized_unop_batching_rule, neg)\n",
    "\n",
    "def reduce_sum_batching_rule(axis_size, vals_in, dims_in, *, axis):\n",
    "  (x,), (x_bdim,) = vals_in, dims_in\n",
    "  new_axis = tuple(ax + (x_bdim <= ax) for ax in axis)\n",
    "  out_bdim = x_bdim - sum(ax < x_bdim for ax in axis)\n",
    "  return [reduce_sum(x, new_axis)], [out_bdim]\n",
    "vmap_rules[reduce_sum_p] = reduce_sum_batching_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vmap_flat(f, in_axes, *args):\n",
    "  axis_size, = {x.shape[ax] for x, ax in zip(args, in_axes)\n",
    "                if ax is not not_mapped}\n",
    "  with new_main(BatchTrace, axis_size) as main:\n",
    "    trace = BatchTrace(main)\n",
    "    tracers_in = [BatchTracer(trace, x, ax) if ax is not None else x\n",
    "                  for x, ax in zip(args, in_axes)]\n",
    "    outs = f(*tracers_in)\n",
    "    tracers_out = [full_raise(trace, out) for out in outs]\n",
    "    vals_out, bdims_out = unzip2((t.val, t.batch_dim) for t in tracers_out)\n",
    "  outs_transposed = [move_batch_axis(axis_size, bdim, 0, val_out)\n",
    "                     for val_out, bdim in zip(vals_out, bdims_out)]\n",
    "  return outs_transposed\n",
    "\n",
    "def vmap(f, in_axes):\n",
    "  def batched_f(*args):\n",
    "    args_flat, in_tree = tree_flatten(args)\n",
    "    in_axes_flat, in_tree2 = tree_flatten(in_axes)\n",
    "    if in_tree != in_tree2: raise TypeError\n",
    "    f_flat, out_tree = flatten_fun(f, in_tree)\n",
    "    outs_flat = vmap_flat(f_flat, in_axes_flat, *args_flat)\n",
    "    return tree_unflatten(out_tree(), outs_flat)\n",
    "  return batched_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        , -0.        ],\n",
       "       [ 0.        ,  0.54030231, -0.        ],\n",
       "       [ 0.        ,  0.        , -0.41614684]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jacfwd(f, x):\n",
    "  pushfwd = lambda v: jvp(f, (x,), (v,))[1]\n",
    "  vecs_in = np.eye(np.size(x)).reshape(np.shape(x) * 2)\n",
    "  return vmap(pushfwd, (0,))(vecs_in)\n",
    "\n",
    "def f(x):\n",
    "  return sin(x)\n",
    "\n",
    "jacfwd(f, np.arange(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAXPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Var:\n",
    "  aval: ShapedArray\n",
    "  def __init__(self, aval): self.aval = aval\n",
    "\n",
    "class Lit:\n",
    "  val: Any\n",
    "  aval: ShapedArray\n",
    "\n",
    "  def __init__(self, val):\n",
    "    self.aval = aval = raise_to_shaped(get_aval(val))\n",
    "    self.val = np.array(val, aval.dtype)\n",
    "\n",
    "Atom = Union[Var, Lit]\n",
    "\n",
    "class JaxprEqn(NamedTuple):\n",
    "  primitive: Primitive\n",
    "  inputs: list[Atom]\n",
    "  params: dict[str, Any]\n",
    "  out_binders: list[Var]\n",
    "\n",
    "class Jaxpr(NamedTuple):\n",
    "  in_binders: list[Var]\n",
    "  eqns: list[JaxprEqn]\n",
    "  outs: list[Atom]\n",
    "\n",
    "  def __hash__(self): return id(self)\n",
    "  __eq__ = op.is_\n",
    "\n",
    "def raise_to_shaped(aval):\n",
    "  return ShapedArray(aval.shape, aval.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxprType(NamedTuple):\n",
    "  in_types:  list[ShapedArray]\n",
    "  out_types: list[ShapedArray]\n",
    "\n",
    "  def __repr__(self):\n",
    "    in_types = ', '.join(aval.str_short() for aval in self.in_types)\n",
    "    out_types = ', '.join(aval.str_short() for aval in self.out_types)\n",
    "    return f'({in_types}) -> ({out_types})'\n",
    "\n",
    "def typecheck_jaxpr(jaxpr: Jaxpr) -> JaxprType:\n",
    "  env: set[Var] = set()\n",
    "\n",
    "  for v in jaxpr.in_binders:\n",
    "    if v in env: raise TypeError\n",
    "    env.add(v)\n",
    "\n",
    "  for eqn in jaxpr.eqns:\n",
    "    in_types = [typecheck_atom(env, x) for x in eqn.inputs]\n",
    "    out_types = abstract_eval_rules[eqn.primitive](*in_types, **eqn.params)\n",
    "    for out_binder, out_type in zip(eqn.out_binders, out_types):\n",
    "      if not out_type == out_binder.aval: raise TypeError\n",
    "    for out_binder in eqn.out_binders:\n",
    "      if out_binder in env: raise TypeError\n",
    "      env.add(out_binder)\n",
    "\n",
    "  in_types = [v.aval for v in jaxpr.in_binders]\n",
    "  out_types = [typecheck_atom(env, x) for x in jaxpr.outs]\n",
    "  return JaxprType(in_types, out_types)\n",
    "\n",
    "def typecheck_atom(env: set[Var], x: Atom) -> ShapedArray:\n",
    "  if isinstance(x, Var):\n",
    "    if x not in env: raise TypeError(\"unbound variable\")\n",
    "    return x.aval\n",
    "  elif isinstance(x, Lit):\n",
    "    return raise_to_shaped(get_aval(x.val))\n",
    "  else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_jaxpr(jaxpr: Jaxpr, args: list[Any]) -> list[Any]:\n",
    "  env: dict[Var, Any] = {}\n",
    "\n",
    "  def read(x: Atom) -> Any:\n",
    "    return env[x] if type(x) is Var else x.val\n",
    "\n",
    "  def write(v: Var, val: Any) -> None:\n",
    "    assert v not in env  # single-assignment\n",
    "    env[v] = val\n",
    "\n",
    "  map(write, jaxpr.in_binders, args)\n",
    "  for eqn in jaxpr.eqns:\n",
    "    in_vals = map(read, eqn.inputs)\n",
    "    outs = bind(eqn.primitive, *in_vals, **eqn.params)\n",
    "    map(write, eqn.out_binders, outs)\n",
    "  return map(read, jaxpr.outs)\n",
    "\n",
    "def jaxpr_as_fun(jaxpr: Jaxpr):\n",
    "  return lambda *args: eval_jaxpr(jaxpr, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst: list[Any], n: int) -> tuple[list[Any], list[Any]]:\n",
    "  assert 0 <= n <= len(lst)\n",
    "  return lst[:n], lst[n:]\n",
    "\n",
    "def partition_list(bs: list[bool], l: list[Any]) -> tuple[list[Any], list[Any]]:\n",
    "  assert len(bs) == len(l)\n",
    "  lists = lst1, lst2 = [], []\n",
    "  for b, x in zip(bs, l):\n",
    "    lists[b].append(x)\n",
    "  return lst1, lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: the analogous class in JAX is called 'DynamicJaxprTracer'\n",
    "class JaxprTracer(Tracer):\n",
    "  __slots__ = ['aval']\n",
    "  aval: ShapedArray\n",
    "\n",
    "  def __init__(self, trace, aval):\n",
    "    self._trace = trace\n",
    "    self.aval = aval\n",
    "\n",
    "# NB: the analogous class in JAX is called 'DynamicJaxprTrace'\n",
    "class JaxprTrace(Trace):\n",
    "  def new_arg(self, aval: ShapedArray) -> JaxprTracer:\n",
    "    aval = raise_to_shaped(aval)\n",
    "    tracer = self.builder.new_tracer(self, aval)\n",
    "    self.builder.tracer_to_var[id(tracer)] = Var(aval)\n",
    "    return tracer\n",
    "\n",
    "  def get_or_make_const_tracer(self, val: Any) -> JaxprTracer:\n",
    "    tracer = self.builder.const_tracers.get(id(val))\n",
    "    if tracer is None:\n",
    "      tracer = self.builder.new_tracer(self, raise_to_shaped(get_aval(val)))\n",
    "      self.builder.add_const(tracer, val)\n",
    "    return tracer\n",
    "  pure = lift = get_or_make_const_tracer\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    avals_in = [t.aval for t in tracers]\n",
    "    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n",
    "    out_tracers = [self.builder.new_tracer(self, a) for a in avals_out]\n",
    "    inputs = [self.builder.getvar(t) for t in tracers]\n",
    "    outvars = [self.builder.add_var(t) for t in out_tracers]\n",
    "    self.builder.add_eqn(JaxprEqn(primitive, inputs, params, outvars))\n",
    "    return out_tracers\n",
    "\n",
    "  @property\n",
    "  def builder(self):\n",
    "    return self.main.global_data\n",
    "\n",
    "# NB: in JAX, we instead attach abstract eval rules to Primitive instances\n",
    "abstract_eval_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxprBuilder:\n",
    "  eqns: list[JaxprEqn]\n",
    "  tracer_to_var: dict[int, Var]\n",
    "  const_tracers: dict[int, JaxprTracer]\n",
    "  constvals: dict[Var, Any]\n",
    "  tracers: list[JaxprTracer]\n",
    "\n",
    "  def __init__(self):\n",
    "    self.eqns = []\n",
    "    self.tracer_to_var = {}\n",
    "    self.const_tracers = {}\n",
    "    self.constvals = {}\n",
    "    self.tracers = []\n",
    "\n",
    "  def new_tracer(self, trace: JaxprTrace, aval: ShapedArray) -> JaxprTracer:\n",
    "    tracer = JaxprTracer(trace, aval)\n",
    "    self.tracers.append(tracer)\n",
    "    return tracer\n",
    "\n",
    "  def add_eqn(self, eqn: JaxprEqn) -> None:\n",
    "    self.eqns.append(eqn)\n",
    "\n",
    "  def add_var(self, tracer: JaxprTracer) -> Var:\n",
    "    assert id(tracer) not in self.tracer_to_var\n",
    "    var = self.tracer_to_var[id(tracer)] = Var(tracer.aval)\n",
    "    return var\n",
    "\n",
    "  def getvar(self, tracer: JaxprTracer) -> Var:\n",
    "    var = self.tracer_to_var.get(id(tracer))\n",
    "    assert var is not None\n",
    "    return var\n",
    "\n",
    "  def add_const(self, tracer: JaxprTracer, val: Any) -> Var:\n",
    "    var = self.add_var(tracer)\n",
    "    self.const_tracers[id(val)] = tracer\n",
    "    self.constvals[var] = val\n",
    "    return var\n",
    "\n",
    "  def build(self, in_tracers: list[JaxprTracer], out_tracers: list[JaxprTracer]\n",
    "            ) -> tuple[Jaxpr, list[Any]]:\n",
    "    constvars, constvals = unzip2(self.constvals.items())\n",
    "    t2v = lambda t: self.tracer_to_var[id(t)]\n",
    "    in_binders = constvars + [t2v(t) for t in in_tracers]\n",
    "    out_vars = [t2v(t) for t in out_tracers]\n",
    "    jaxpr = Jaxpr(in_binders, self.eqns, out_vars)\n",
    "    typecheck_jaxpr(jaxpr)\n",
    "    jaxpr, constvals = _inline_literals(jaxpr, constvals)\n",
    "    return jaxpr, constvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inline_literals(jaxpr: Jaxpr, consts: list[Any]) -> tuple[Jaxpr, list[Any]]:\n",
    "  const_binders, other_binders = split_list(jaxpr.in_binders, len(consts))\n",
    "  scalars = [type(x) in jax_types and not get_aval(x).shape for x in consts]\n",
    "  new_const_binders, lit_binders = partition_list(scalars, const_binders)\n",
    "  new_consts, lit_vals = partition_list(scalars, consts)\n",
    "  literals = dict(zip(lit_binders, map(Lit, lit_vals)))\n",
    "  new_eqns = [JaxprEqn(eqn.primitive, [literals.get(x, x) for x in eqn.inputs],\n",
    "                       eqn.params, eqn.out_binders) for eqn in jaxpr.eqns]\n",
    "  new_outs = [literals.get(x, x) for x in jaxpr.outs]\n",
    "  new_jaxpr = Jaxpr(new_const_binders + other_binders, new_eqns, new_outs)\n",
    "  typecheck_jaxpr(new_jaxpr)\n",
    "  return new_jaxpr, new_consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binop_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n",
    "  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n",
    "    raise TypeError\n",
    "  if raise_to_shaped(x) != raise_to_shaped(y): raise TypeError\n",
    "  return [ShapedArray(x.shape, x.dtype)]\n",
    "\n",
    "abstract_eval_rules[add_p] = binop_abstract_eval\n",
    "abstract_eval_rules[mul_p] = binop_abstract_eval\n",
    "\n",
    "def compare_abstract_eval(x: ShapedArray, y: ShapedArray) -> list[ShapedArray]:\n",
    "  if not isinstance(x, ShapedArray) or not isinstance(y, ShapedArray):\n",
    "    raise TypeError\n",
    "  if x.shape != y.shape: raise TypeError\n",
    "  return [ShapedArray(x.shape, np.dtype('bool'))]\n",
    "abstract_eval_rules[greater_p] = compare_abstract_eval\n",
    "abstract_eval_rules[less_p] = compare_abstract_eval\n",
    "\n",
    "def vectorized_unop_abstract_eval(x: ShapedArray) -> list[ShapedArray]:\n",
    "  return [ShapedArray(x.shape, x.dtype)]\n",
    "\n",
    "abstract_eval_rules[sin_p] = vectorized_unop_abstract_eval\n",
    "abstract_eval_rules[cos_p] = vectorized_unop_abstract_eval\n",
    "abstract_eval_rules[neg_p] = vectorized_unop_abstract_eval\n",
    "abstract_eval_rules[exp_p] = vectorized_unop_abstract_eval\n",
    "\n",
    "\n",
    "def reduce_sum_abstract_eval(x: ShapedArray, *, axis: tuple[int, ...]\n",
    "                             ) -> list[ShapedArray]:\n",
    "  axis_ = set(axis)\n",
    "  new_shape = [d for i, d in enumerate(x.shape) if i not in axis_]\n",
    "  return [ShapedArray(tuple(new_shape), x.dtype)]\n",
    "abstract_eval_rules[reduce_sum_p] = reduce_sum_abstract_eval\n",
    "\n",
    "def broadcast_abstract_eval(x: ShapedArray, *, shape: Sequence[int],\n",
    "                            axes: Sequence[int]) -> list[ShapedArray]:\n",
    "  return [ShapedArray(tuple(shape), x.dtype)]\n",
    "abstract_eval_rules[broadcast_p] = broadcast_abstract_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()  # ShapedArrays are hashable\n",
    "def make_jaxpr_v1(f, *avals_in):\n",
    "  avals_in, in_tree = tree_flatten(avals_in)\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "\n",
    "  builder = JaxprBuilder()\n",
    "  with new_main(JaxprTrace, builder) as main:\n",
    "    trace = JaxprTrace(main)\n",
    "    tracers_in = [trace.new_arg(aval) for aval in avals_in]\n",
    "    outs = f(*tracers_in)\n",
    "    tracers_out = [full_raise(trace, out) for out in outs]\n",
    "    jaxpr, consts = builder.build(tracers_in, tracers_out)\n",
    "  return jaxpr, consts, out_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "class PPrint:\n",
    "  lines: list[tuple[int, str]]\n",
    "\n",
    "  def __init__(self, lines):\n",
    "    self.lines = lines\n",
    "\n",
    "  def indent(self, indent: int) -> 'PPrint':\n",
    "    return PPrint([(indent + orig_indent, s) for orig_indent, s in self.lines])\n",
    "\n",
    "  def __add__(self, rhs: 'PPrint') -> 'PPrint':\n",
    "    return PPrint(self.lines + rhs.lines)\n",
    "\n",
    "  def __rshift__(self, rhs: 'PPrint') -> 'PPrint':\n",
    "    if not rhs.lines: return self\n",
    "    if not self.lines: return rhs\n",
    "    indent, s = self.lines[-1]\n",
    "    indented_block = rhs.indent(indent + len(s))\n",
    "    common_line = s + ' ' * rhs.lines[0][0] + rhs.lines[0][1]\n",
    "    return PPrint(self.lines[:-1]\n",
    "                  + [(indent, common_line)]\n",
    "                  + indented_block.lines[1:])\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    return '\\n'.join(' ' * indent + s for indent, s in self.lines)\n",
    "\n",
    "def pp(s: Any) -> PPrint:\n",
    "  return PPrint([(0, line) for line in str(s).splitlines()])\n",
    "\n",
    "def vcat(ps: list[PPrint]) -> PPrint:\n",
    "  return sum(ps, pp(''))\n",
    "\n",
    "def pp_jaxpr(jaxpr: Jaxpr) -> PPrint:\n",
    "  namegen = (''.join(s) for r in it.count(1)\n",
    "             for s in it.permutations(string.ascii_lowercase, r))\n",
    "  names = defaultdict(lambda: next(namegen))\n",
    "  in_binders = ', '.join(var_str(names, x) for x in jaxpr.in_binders)\n",
    "  eqns = vcat([pp_eqn(names, e) for e in jaxpr.eqns])\n",
    "  outs = ', '.join(names[v] if isinstance(v, Var) else str(v.val)\n",
    "                   for v in jaxpr.outs)\n",
    "  return (pp(f'{{ lambda {in_binders} .') +\n",
    "          ((pp('let ') >> eqns) + pp(f'in ( {outs} ) }}')).indent(2))\n",
    "\n",
    "def var_str(names: defaultdict[Var, str], v: Var) -> str:\n",
    "  return f'{names[v]}:{v.aval.str_short()}'\n",
    "\n",
    "def pp_eqn(names: defaultdict[Var, str], eqn: JaxprEqn) -> PPrint:\n",
    "  rule = pp_rules.get(eqn.primitive)\n",
    "  if rule:\n",
    "    return rule(names, eqn)\n",
    "  else:\n",
    "    lhs = pp(' '.join(var_str(names, v) for v in eqn.out_binders))\n",
    "    rhs = (pp(eqn.primitive.name) >> pp_params(eqn.params) >>\n",
    "           pp(' '.join(names[x] if isinstance(x, Var) else str(x.val)\n",
    "                       for x in eqn.inputs)))\n",
    "    return lhs >> pp(' = ') >> rhs\n",
    "\n",
    "def pp_params(params: dict[str, Any]) -> PPrint:\n",
    "  items = sorted(params.items())\n",
    "  if items:\n",
    "    return pp(' [ ') >> vcat([pp(f'{k}={v}') for k, v in items]) >> pp(' ] ')\n",
    "  else:\n",
    "    return pp(' ')\n",
    "\n",
    "Jaxpr.__repr__ = lambda self: str(pp_jaxpr(self))\n",
    "pp_rules: dict[Primitive, Callable[..., PPrint]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda a:float64[] .\n",
      "  let b:float64[] = mul 2.0 a\n",
      "  in ( b ) }\n",
      "(float64[]) -> (float64[])\n"
     ]
    }
   ],
   "source": [
    "jaxpr, consts, _ = make_jaxpr_v1(lambda x: 2. * x, raise_to_shaped(get_aval(3.)))\n",
    "print(jaxpr)\n",
    "print(typecheck_jaxpr(jaxpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda  .\n",
      "  let \n",
      "  in ( 4.0 ) }\n"
     ]
    }
   ],
   "source": [
    "jaxpr, consts, _ = make_jaxpr_v1(lambda: mul(2., 2.))\n",
    "print(jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda  .\n",
      "  let a:float64[] = mul 2.0 2.0\n",
      "  in ( a ) }\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def new_dynamic(main: MainTrace):\n",
    "  global dynamic_trace\n",
    "  prev_dynamic_trace, dynamic_trace = dynamic_trace, main\n",
    "  try:\n",
    "    yield\n",
    "  finally:\n",
    "    dynamic_trace = prev_dynamic_trace\n",
    "\n",
    "@lru_cache()\n",
    "def make_jaxpr(f: Callable, *avals_in: ShapedArray,\n",
    "               ) -> tuple[Jaxpr, list[Any], PyTreeDef]:\n",
    "  avals_in, in_tree = tree_flatten(avals_in)\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "\n",
    "  builder = JaxprBuilder()\n",
    "  with new_main(JaxprTrace, builder) as main:\n",
    "    with new_dynamic(main):\n",
    "      trace = JaxprTrace(main)\n",
    "      tracers_in = [trace.new_arg(aval) for aval in avals_in]\n",
    "      outs = f(*tracers_in)\n",
    "      tracers_out = [full_raise(trace, out) for out in outs]\n",
    "      jaxpr, consts = builder.build(tracers_in, tracers_out)\n",
    "  return jaxpr, consts, out_tree()\n",
    "\n",
    "jaxpr, consts, _ = make_jaxpr(lambda: mul(2., 2.))\n",
    "print(jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jit(f):\n",
    "  def f_jitted(*args):\n",
    "    avals_in = [raise_to_shaped(get_aval(x)) for x in args]\n",
    "    jaxpr, consts, out_tree = make_jaxpr(f, *avals_in)\n",
    "    outs = bind(xla_call_p, *consts, *args, jaxpr=jaxpr, num_consts=len(consts))\n",
    "    return tree_unflatten(out_tree, outs)\n",
    "  return f_jitted\n",
    "\n",
    "xla_call_p = Primitive('xla_call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDHashable:\n",
    "  val: Any\n",
    "\n",
    "  def __init__(self, val):\n",
    "    self.val = val\n",
    "\n",
    "  def __hash__(self) -> int:\n",
    "    return id(self.val)\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    return type(other) is IDHashable and id(self.val) == id(other.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax._src import xla_bridge as xb\n",
    "from jax._src.lib import xla_client as xc\n",
    "xe = xc._xla\n",
    "xops = xc._xla.ops\n",
    "\n",
    "def xla_call_impl(*args, jaxpr: Jaxpr, num_consts: int):\n",
    "  consts, args = args[:num_consts], args[num_consts:]\n",
    "  hashable_consts = tuple(map(IDHashable, consts))\n",
    "  execute = xla_callable(IDHashable(jaxpr), hashable_consts)\n",
    "  return execute(*args)\n",
    "impl_rules[xla_call_p] = xla_call_impl\n",
    "\n",
    "@lru_cache()\n",
    "def xla_callable(hashable_jaxpr: IDHashable,\n",
    "                 hashable_consts: tuple[IDHashable, ...]):\n",
    "  jaxpr: Jaxpr = hashable_jaxpr.val\n",
    "  typecheck_jaxpr(jaxpr)\n",
    "  consts = [x.val for x in hashable_consts]\n",
    "  in_avals = [v.aval for v in jaxpr.in_binders[len(consts):]]\n",
    "  c = xc.XlaBuilder('xla_call')\n",
    "  xla_consts = _xla_consts(c, consts)\n",
    "  xla_params = _xla_params(c, in_avals)\n",
    "  outs = jaxpr_subcomp(c, jaxpr, xla_consts + xla_params)\n",
    "  out = xops.Tuple(c, outs)\n",
    "  compiled = xb.get_backend(None).compile(\n",
    "    xc._xla.mlir.xla_computation_to_mlir_module(c.build(out)))\n",
    "  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n",
    "\n",
    "def _xla_consts(c: xe.XlaBuilder, consts: list[Any]) -> list[xe.XlaOp]:\n",
    "  unique_consts = {id(cnst): cnst for cnst in consts}\n",
    "  xla_consts = {\n",
    "      id_: xops.ConstantLiteral(c, cnst) for id_, cnst in unique_consts.items()}\n",
    "  return [xla_consts[id(cnst)] for cnst in consts]\n",
    "\n",
    "def _xla_params(c: xe.XlaBuilder, avals_in: list[ShapedArray]) -> list[xe.XlaOp]:\n",
    "  return [xops.Parameter(c, i, _xla_shape(a)) for i, a in enumerate(avals_in)]\n",
    "\n",
    "def _xla_shape(aval: ShapedArray) -> xe.Shape:\n",
    "  return xc.Shape.array_shape(xc.dtype_to_etype(aval.dtype), aval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 15:18:08.957794: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16899571712\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "def jaxpr_subcomp(c: xe.XlaBuilder, jaxpr: Jaxpr, args: list[xe.XlaOp]\n",
    "                  ) -> list[xe.XlaOp]:\n",
    "  env: dict[Var, xe.XlaOp] = {}\n",
    "\n",
    "  def read(x: Atom) -> xe.XlaOp:\n",
    "    return env[x] if type(x) is Var else xops.Constant(c, np.asarray(x.val))\n",
    "\n",
    "  def write(v: Var, val: xe.XlaOp) -> None:\n",
    "    env[v] = val\n",
    "\n",
    "  map(write, jaxpr.in_binders, args)\n",
    "  for eqn in jaxpr.eqns:\n",
    "    in_avals = [x.aval for x in eqn.inputs]\n",
    "    in_vals = map(read, eqn.inputs)\n",
    "    rule = xla_translations[eqn.primitive]\n",
    "    out_vals = rule(c, in_avals, in_vals, **eqn.params)\n",
    "    map(write, eqn.out_binders, out_vals)\n",
    "  return map(read, jaxpr.outs)\n",
    "\n",
    "def execute_compiled(compiled, out_avals, *args):\n",
    "  input_bufs = [input_handlers[type(x)](x) for x in args]\n",
    "  out_bufs = compiled.execute(input_bufs)\n",
    "  return [handle_result(aval, buf) for aval, buf in zip(out_avals, out_bufs)]\n",
    "\n",
    "default_input_handler = xb.get_backend(None).buffer_from_pyval\n",
    "input_handlers = {ty: default_input_handler for ty in\n",
    "                  [bool, int, float, np.ndarray, np.float64, np.float32]}\n",
    "\n",
    "def handle_result(aval: ShapedArray, buf):\n",
    "  del aval  # Unused for now\n",
    "  return np.asarray(buf)\n",
    "\n",
    "xla_translations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_translation(op, c, in_avals, in_vals):\n",
    "  del c, in_avals\n",
    "  return [op(*in_vals)]\n",
    "\n",
    "xla_translations[add_p] = partial(direct_translation, xops.Add)\n",
    "xla_translations[mul_p] = partial(direct_translation, xops.Mul)\n",
    "xla_translations[neg_p] = partial(direct_translation, xops.Neg)\n",
    "xla_translations[sin_p] = partial(direct_translation, xops.Sin)\n",
    "xla_translations[cos_p] = partial(direct_translation, xops.Cos)\n",
    "\n",
    "xla_translations[exp_p] = partial(direct_translation, xops.Exp)\n",
    "\n",
    "xla_translations[greater_p] = partial(direct_translation, xops.Gt)\n",
    "xla_translations[less_p] = partial(direct_translation, xops.Lt)\n",
    "\n",
    "def reduce_sum_translation(c, in_avals, in_vals, *, axis):\n",
    "  (x_aval,), (x,) = in_avals, in_vals\n",
    "  zero = xops.ConstantLiteral(c, np.array(0, x_aval.dtype))\n",
    "  subc = xc.XlaBuilder('add')\n",
    "  shape = _xla_shape(ShapedArray((), x_aval.dtype))\n",
    "  xops.Add(xops.Parameter(subc, 0, shape), xops.Parameter(subc, 1, shape))\n",
    "  return [xops.Reduce(c, [x], [zero], subc.build(), axis)]\n",
    "xla_translations[reduce_sum_p] = reduce_sum_translation\n",
    "\n",
    "def broadcast_translation(c, in_avals, in_vals, *, shape, axes):\n",
    "  x, = in_vals\n",
    "  dims_complement = [i for i in range(len(shape)) if i not in axes]\n",
    "  return [xops.BroadcastInDim(x, shape, dims_complement)]\n",
    "xla_translations[broadcast_p] = broadcast_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_call_jvp_rule(primals, tangents, *, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  new_jaxpr, new_consts = jvp_jaxpr(jaxpr)\n",
    "  outs = bind(xla_call_p, *new_consts, *primals, *tangents, jaxpr=new_jaxpr,\n",
    "              num_consts=len(new_consts))\n",
    "  n = len(outs) // 2\n",
    "  primals_out, tangents_out = outs[:n], outs[n:]\n",
    "  return primals_out, tangents_out\n",
    "jvp_rules[xla_call_p] = xla_call_jvp_rule\n",
    "\n",
    "@lru_cache()\n",
    "def jvp_jaxpr(jaxpr: Jaxpr) -> tuple[Jaxpr, list[Any]]:\n",
    "  def jvp_traceable(*primals_and_tangents):\n",
    "    n = len(primals_and_tangents) // 2\n",
    "    primals, tangents = primals_and_tangents[:n], primals_and_tangents[n:]\n",
    "    return jvp(jaxpr_as_fun(jaxpr), primals, tangents)\n",
    "\n",
    "  in_avals = [v.aval for v in jaxpr.in_binders]\n",
    "  new_jaxpr, new_consts, _ = make_jaxpr(jvp_traceable, *in_avals, *in_avals)\n",
    "  return new_jaxpr, new_consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_call_vmap_rule(axis_size, vals_in, dims_in, *, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  new_jaxpr, new_consts = vmap_jaxpr(jaxpr, axis_size, tuple(dims_in))\n",
    "  outs = bind(xla_call_p, *new_consts, *vals_in, jaxpr=new_jaxpr,\n",
    "              num_consts=len(new_consts))\n",
    "  return outs, [0] * len(outs)\n",
    "vmap_rules[xla_call_p] = xla_call_vmap_rule\n",
    "\n",
    "@lru_cache()\n",
    "def vmap_jaxpr(jaxpr: Jaxpr, axis_size: int, bdims_in: tuple[BatchAxis, ...]\n",
    "               ) -> tuple[Jaxpr, list[Any]]:\n",
    "  vmap_traceable = vmap(jaxpr_as_fun(jaxpr), tuple(bdims_in))\n",
    "  in_avals = [unmapped_aval(axis_size, d, v.aval)\n",
    "              for v, d in zip(jaxpr.in_binders, bdims_in)]\n",
    "  new_jaxpr, new_consts, _ = make_jaxpr(vmap_traceable, *in_avals)\n",
    "  return new_jaxpr, new_consts\n",
    "\n",
    "def unmapped_aval(axis_size: int, batch_dim: BatchAxis, aval: ShapedArray\n",
    "                  ) -> ShapedArray:\n",
    "  if batch_dim is not_mapped:\n",
    "    return aval\n",
    "  else:\n",
    "    shape = list(aval.shape)\n",
    "    shape.insert(batch_dim, axis_size)\n",
    "    return ShapedArray(tuple(shape), aval.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_call_abstract_eval_rule(*in_types, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  jaxpr_type = typecheck_jaxpr(jaxpr)\n",
    "  if not all(t1 == t2 for t1, t2 in zip(jaxpr_type.in_types, in_types)):\n",
    "    raise TypeError\n",
    "  return jaxpr_type.out_types\n",
    "abstract_eval_rules[xla_call_p] = xla_call_abstract_eval_rule\n",
    "\n",
    "def xla_call_translation(c, in_avals, in_vals, *, jaxpr, num_consts):\n",
    "  del num_consts  # Only used at top-level.\n",
    "  # Calling jaxpr_subcomp directly would inline. We generate a Call HLO instead.\n",
    "  subc = xc.XlaBuilder('inner xla_call')\n",
    "  xla_params = _xla_params(subc, in_avals)\n",
    "  outs = jaxpr_subcomp(subc, jaxpr, xla_params)\n",
    "  subc = subc.build(xops.Tuple(subc, outs))\n",
    "  return destructure_tuple(c, xops.Call(c, subc, in_vals))\n",
    "xla_translations[xla_call_p] = xla_call_translation\n",
    "\n",
    "def destructure_tuple(c, tup):\n",
    "  num_elements = len(c.get_shape(tup).tuple_shapes())\n",
    "  return [xops.GetTupleElement(tup, i) for i in range(num_elements)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing!\n",
      "2.7177599838802657\n",
      "2.979984993200891\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "  print('tracing!')\n",
    "  y = sin(x) * 2.\n",
    "  z = - y + x\n",
    "  return z\n",
    "\n",
    "x, xdot = 3., 1.\n",
    "y, ydot = jvp(f, (x,), (xdot,))\n",
    "print(y)\n",
    "print(ydot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_result(aval: ShapedArray, buf):  # noqa: F811\n",
    "  return Array(aval, buf)\n",
    "\n",
    "class Array:\n",
    "  buf: Any\n",
    "  aval: ShapedArray\n",
    "\n",
    "  def __init__(self, aval, buf):\n",
    "    self.aval = aval\n",
    "    self.buf = buf\n",
    "\n",
    "  dtype = property(lambda self: self.aval.dtype)\n",
    "  shape = property(lambda self: self.aval.shape)\n",
    "  ndim  = property(lambda self: self.aval.ndim)\n",
    "\n",
    "  def __array__(self): return np.asarray(self.buf)\n",
    "  def __repr__(self):  return repr(np.asarray(self.buf))\n",
    "  def __str__(self):   return str(np.asarray(self.buf))\n",
    "\n",
    "  _neg = staticmethod(neg)\n",
    "  _add = staticmethod(add)\n",
    "  _radd = staticmethod(add)\n",
    "  _mul = staticmethod(mul)\n",
    "  _rmul = staticmethod(mul)\n",
    "  _gt = staticmethod(greater)\n",
    "  _lt = staticmethod(less)\n",
    "input_handlers[Array] = lambda x: x.buf\n",
    "\n",
    "jax_types.add(Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearize and VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_half(lst: list[Any]) -> tuple[list[Any], list[Any]]:\n",
    "  assert not len(lst) % 2\n",
    "  return split_list(lst, len(lst) // 2)\n",
    "\n",
    "def merge_lists(which: list[bool], l1: list[Any], l2: list[Any]) -> list[Any]:\n",
    "  l1, l2 = iter(l1), iter(l2)\n",
    "  out = [next(l2) if b else next(l1) for b in which]\n",
    "  assert next(l1, None) is next(l2, None) is None\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_flat(f, *primals_in):\n",
    "  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n",
    "              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n",
    "  def f_jvp(*primals_tangents_in):\n",
    "    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n",
    "    return [*primals_out, *tangents_out]\n",
    "  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)\n",
    "  primal_pvals, _ = split_half(pvals_out)\n",
    "  assert all(pval.is_known for pval in primal_pvals)\n",
    "  primals_out = [pval.const for pval in primal_pvals]\n",
    "  f_lin = lambda *tangents: eval_jaxpr(jaxpr, [*consts, *tangents])\n",
    "  return primals_out, f_lin\n",
    "\n",
    "def linearize(f, *primals_in):\n",
    "  primals_in_flat, in_tree = tree_flatten(primals_in)\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "  primals_out_flat, f_lin_flat = linearize_flat(f, *primals_in_flat)\n",
    "  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n",
    "\n",
    "  def f_lin(*tangents_in):\n",
    "    tangents_in_flat, in_tree2 = tree_flatten(tangents_in)\n",
    "    if in_tree != in_tree2: raise TypeError\n",
    "    tangents_out_flat = f_lin_flat(*tangents_in_flat)\n",
    "    return tree_unflatten(out_tree(), tangents_out_flat)\n",
    "\n",
    "  return primals_out, f_lin\n",
    "\n",
    "def vspace(aval: ShapedArray) -> ShapedArray:\n",
    "  return raise_to_shaped(aval)  # TODO handle integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialVal(NamedTuple):\n",
    "  aval: ShapedArray\n",
    "  const: Optional[Any]\n",
    "\n",
    "  @classmethod\n",
    "  def known(cls, val: Any):\n",
    "    return PartialVal(get_aval(val), val)\n",
    "\n",
    "  @classmethod\n",
    "  def unknown(cls, aval: ShapedArray):\n",
    "    return PartialVal(aval, None)\n",
    "\n",
    "  is_known   = property(lambda self: self.const is not None)\n",
    "  is_unknown = property(lambda self: self.const is     None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_eval_flat(f: Callable, pvals_in: list[PartialVal]\n",
    "                      ) -> tuple[Jaxpr, list[PartialVal], list[Any]]:\n",
    "  with new_main(PartialEvalTrace) as main:\n",
    "    trace = PartialEvalTrace(main)\n",
    "    tracers_in = [trace.new_arg(pval) for pval in pvals_in]\n",
    "    outs = f(*tracers_in)\n",
    "    tracers_out = [full_raise(trace, out) for out in outs]\n",
    "    pvals_out = [t.pval for t in tracers_out]\n",
    "    unk_tracers_in  = [t for t in tracers_in  if t.pval.is_unknown]\n",
    "    unk_tracers_out = [t for t in tracers_out if t.pval.is_unknown]\n",
    "    jaxpr, consts = tracers_to_jaxpr(unk_tracers_in, unk_tracers_out)\n",
    "  return jaxpr, pvals_out, consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weakref import ref, ReferenceType\n",
    "\n",
    "class LambdaBindingRecipe(NamedTuple):\n",
    "  pass\n",
    "\n",
    "class ConstRecipe(NamedTuple):\n",
    "  val: Any\n",
    "\n",
    "class JaxprEqnRecipe(NamedTuple):\n",
    "  prim: Primitive\n",
    "  tracers_in: list['PartialEvalTracer']\n",
    "  params: dict[str, Any]\n",
    "  avals_out: list[ShapedArray]\n",
    "  tracer_refs_out: list['ReferenceType[PartialEvalTracer]']\n",
    "\n",
    "JaxprRecipe = Union[LambdaBindingRecipe, ConstRecipe, JaxprEqnRecipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialEvalTracer(Tracer):\n",
    "  pval: PartialVal\n",
    "  recipe: Optional[JaxprRecipe]\n",
    "\n",
    "  def __init__(self, trace, pval, recipe):\n",
    "    self._trace = trace\n",
    "    self.pval = pval\n",
    "    self.recipe = recipe\n",
    "\n",
    "  aval = property(lambda self: self.pval.aval)\n",
    "\n",
    "  def full_lower(self):\n",
    "    if self.pval.is_known:\n",
    "      return full_lower(self.pval.const)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialEvalTrace(Trace):\n",
    "  def new_arg(self, pval: PartialVal) -> Any:\n",
    "    return PartialEvalTracer(self, pval, LambdaBindingRecipe())\n",
    "\n",
    "  def lift(self, val: Any) -> PartialEvalTracer:\n",
    "    return PartialEvalTracer(self, PartialVal.known(val), None)\n",
    "  pure = lift\n",
    "\n",
    "  def instantiate_const(self, tracer: PartialEvalTracer) -> PartialEvalTracer:\n",
    "    if tracer.pval.is_unknown:\n",
    "      return tracer\n",
    "    else:\n",
    "      pval = PartialVal.unknown(raise_to_shaped(tracer.aval))\n",
    "      return PartialEvalTracer(self, pval, ConstRecipe(tracer.pval.const))\n",
    "\n",
    "  def process_primitive(self, primitive, tracers, params):\n",
    "    if all(t.pval.is_known for t in tracers):\n",
    "      return bind(primitive, *map(full_lower, tracers), **params)\n",
    "    rule = partial_eval_rules.get(primitive)\n",
    "    if rule: return rule(self, tracers, **params)\n",
    "    tracers_in = [self.instantiate_const(t) for t in tracers]\n",
    "    avals_in = [t.aval for t in tracers_in]\n",
    "    avals_out = abstract_eval_rules[primitive](*avals_in, **params)\n",
    "    tracers_out = [PartialEvalTracer(self, PartialVal.unknown(aval), None)\n",
    "                   for aval in avals_out]\n",
    "    eqn = JaxprEqnRecipe(primitive, tracers_in, params, avals_out,\n",
    "                         map(ref, tracers_out))\n",
    "    for t in tracers_out: t.recipe = eqn\n",
    "    return tracers_out\n",
    "\n",
    "partial_eval_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracers_to_jaxpr(tracers_in: list[PartialEvalTracer],\n",
    "                     tracers_out: list[PartialEvalTracer]):\n",
    "  tracer_to_var: dict[int, Var] = {id(t): Var(raise_to_shaped(t.aval))\n",
    "                                   for t in tracers_in}\n",
    "  constvar_to_val: dict[int, Any] = {}\n",
    "  constid_to_var: dict[int, Var] = {}\n",
    "  processed_eqns: set[int] = set()\n",
    "  eqns: list[JaxprEqn] = []\n",
    "  for t in toposort(tracers_out, tracer_parents):\n",
    "    if isinstance(t.recipe, LambdaBindingRecipe):\n",
    "      assert id(t) in set(map(id, tracers_in))\n",
    "    elif isinstance(t.recipe, ConstRecipe):\n",
    "      val = t.recipe.val\n",
    "      var = constid_to_var.get(id(val))\n",
    "      if var is None:\n",
    "        aval = raise_to_shaped(get_aval(val))\n",
    "        var = constid_to_var[id(val)] = Var(aval)\n",
    "        constvar_to_val[var] = val\n",
    "      tracer_to_var[id(t)] = var\n",
    "    elif isinstance(t.recipe, JaxprEqnRecipe):\n",
    "      if id(t.recipe) not in processed_eqns:\n",
    "        eqns.append(recipe_to_eqn(tracer_to_var, t.recipe))\n",
    "        processed_eqns.add(id(t.recipe))\n",
    "    else:\n",
    "      raise TypeError(t.recipe)\n",
    "\n",
    "  constvars, constvals = unzip2(constvar_to_val.items())\n",
    "  in_binders = constvars + [tracer_to_var[id(t)] for t in tracers_in]\n",
    "  out_vars = [tracer_to_var[id(t)] for t in tracers_out]\n",
    "  jaxpr = Jaxpr(in_binders, eqns, out_vars)\n",
    "  typecheck_jaxpr(jaxpr)\n",
    "  return jaxpr, constvals\n",
    "\n",
    "def recipe_to_eqn(tracer_to_var: dict[int, Var], recipe: JaxprEqnRecipe\n",
    "                  ) -> JaxprEqn:\n",
    "  inputs = [tracer_to_var[id(t)] for t in recipe.tracers_in]\n",
    "  out_binders = [Var(aval) for aval in recipe.avals_out]\n",
    "  for t_ref, var in zip(recipe.tracer_refs_out, out_binders):\n",
    "    if t_ref() is not None: tracer_to_var[id(t_ref())] = var\n",
    "  return JaxprEqn(recipe.prim, inputs, recipe.params, out_binders)\n",
    "\n",
    "def tracer_parents(t: PartialEvalTracer) -> list[PartialEvalTracer]:\n",
    "  return t.recipe.tracers_in if isinstance(t.recipe, JaxprEqnRecipe) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toposort(out_nodes: list[Any], parents: Callable[[Any], list[Any]]):\n",
    "  if not out_nodes: return []\n",
    "  out_nodes = remove_duplicates(out_nodes)\n",
    "\n",
    "  child_counts = {}\n",
    "  stack = list(out_nodes)\n",
    "  while stack:\n",
    "    node = stack.pop()\n",
    "    if id(node) in child_counts:\n",
    "      child_counts[id(node)] += 1\n",
    "    else:\n",
    "      child_counts[id(node)] = 1\n",
    "      stack.extend(parents(node))\n",
    "  for node in out_nodes:\n",
    "    child_counts[id(node)] -= 1\n",
    "\n",
    "  sorted_nodes = []\n",
    "  childless_nodes = [node for node in out_nodes if not child_counts[id(node)]]\n",
    "  while childless_nodes:\n",
    "    node = childless_nodes.pop()\n",
    "    sorted_nodes.append(node)\n",
    "    for parent in parents(node):\n",
    "      if child_counts[id(parent)] == 1:\n",
    "        childless_nodes.append(parent)\n",
    "      else:\n",
    "        child_counts[id(parent)] -= 1\n",
    "\n",
    "  sorted_nodes = sorted_nodes[::-1]\n",
    "  check_toposort(sorted_nodes, parents)\n",
    "  return sorted_nodes\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "  seen = set()\n",
    "  return [x for x in lst if id(x) not in seen and not seen.add(id(x))]\n",
    "\n",
    "def check_toposort(nodes: list[Any], parents: Callable[[Any], list[Any]]):\n",
    "  seen = set()\n",
    "  for node in nodes:\n",
    "    assert all(id(parent) in seen for parent in parents(node))\n",
    "    seen.add(id(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1411200080598672 0.1411200080598672\n",
      "-0.9899924966004454 -0.9899924966004454\n"
     ]
    }
   ],
   "source": [
    "y, sin_lin = linearize(sin, 3.)\n",
    "print(y, sin(3.))\n",
    "print(sin_lin(1.), cos(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xla_call_partial_eval(trace, tracers, *, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  in_unknowns = [not t.pval.is_known for t in tracers]\n",
    "  jaxpr1, jaxpr2, out_unknowns, num_res = partial_eval_jaxpr(jaxpr, in_unknowns)\n",
    "  known_tracers, unknown_tracers = partition_list(in_unknowns, tracers)\n",
    "  known_vals = [t.pval.const for t in known_tracers]\n",
    "  outs1_res = bind(xla_call_p, *known_vals, jaxpr=jaxpr1, num_consts=0)\n",
    "  outs1, res = split_list(outs1_res, len(jaxpr1.outs) - num_res)\n",
    "  res_tracers = [trace.instantiate_const(full_raise(trace, x)) for x in res]\n",
    "  outs2 = [PartialEvalTracer(trace, PartialVal.unknown(v.aval), None)\n",
    "           for v in jaxpr2.outs]\n",
    "  eqn = JaxprEqnRecipe(xla_call_p, res_tracers + unknown_tracers,\n",
    "                       dict(jaxpr=jaxpr2, num_consts=0),\n",
    "                       [v.aval for v in jaxpr2.outs], map(ref, outs2))\n",
    "  for t in outs2: t.recipe = eqn\n",
    "  return merge_lists(out_unknowns, outs1, outs2)\n",
    "partial_eval_rules[xla_call_p] = xla_call_partial_eval\n",
    "\n",
    "def partial_eval_jaxpr(jaxpr: Jaxpr, in_unknowns: list[bool],\n",
    "                       instantiate: Optional[list[bool]] = None,\n",
    "                       ) -> tuple[Jaxpr, Jaxpr, list[bool], int]:\n",
    "  env: dict[Var, bool] = {}\n",
    "  residuals: set[Var] = set()\n",
    "\n",
    "  def read(x: Atom) -> bool:\n",
    "    return type(x) is Var and env[x]\n",
    "\n",
    "  def write(unk: bool, v: Var) -> None:\n",
    "    env[v] = unk\n",
    "\n",
    "  def new_res(x: Atom) -> Atom:\n",
    "    if type(x) is Var: residuals.add(x)\n",
    "    return x\n",
    "\n",
    "  eqns1, eqns2 = [], []\n",
    "  map(write, in_unknowns, jaxpr.in_binders)\n",
    "  for eqn in jaxpr.eqns:\n",
    "    unks_in = map(read, eqn.inputs)\n",
    "    rule = partial_eval_jaxpr_rules.get(eqn.primitive)\n",
    "    if rule:\n",
    "      eqn1, eqn2, unks_out, res = rule(unks_in, eqn)\n",
    "      eqns1.append(eqn1); eqns2.append(eqn2); residuals.update(res)\n",
    "      map(write, unks_out, eqn.out_binders)\n",
    "    elif any(unks_in):\n",
    "      inputs = [v if unk else new_res(v) for unk, v in zip(unks_in, eqn.inputs)]\n",
    "      eqns2.append(JaxprEqn(eqn.primitive, inputs, eqn.params, eqn.out_binders))\n",
    "      map(partial(write, True), eqn.out_binders)\n",
    "    else:\n",
    "      eqns1.append(eqn)\n",
    "      map(partial(write, False), eqn.out_binders)\n",
    "  out_unknowns = map(read, jaxpr.outs)\n",
    "  if instantiate is not None:\n",
    "    for v, uk, inst in zip(jaxpr.outs, out_unknowns, instantiate):\n",
    "      if inst and not uk: new_res(v)\n",
    "    out_unknowns = map(op.or_, out_unknowns, instantiate)\n",
    "\n",
    "  residuals, num_res = list(residuals), len(residuals)\n",
    "  assert all(type(v) is Var for v in residuals), residuals\n",
    "\n",
    "  ins1, ins2 = partition_list(in_unknowns, jaxpr.in_binders)\n",
    "  outs1, outs2 = partition_list(out_unknowns, jaxpr.outs)\n",
    "\n",
    "  jaxpr1 = Jaxpr(ins1, eqns1, outs1 + residuals)\n",
    "  jaxpr2 = Jaxpr(residuals + ins2, eqns2, outs2)\n",
    "  typecheck_partial_eval_jaxpr(jaxpr, in_unknowns, out_unknowns, jaxpr1, jaxpr2)\n",
    "\n",
    "  return jaxpr1, jaxpr2, out_unknowns, num_res\n",
    "\n",
    "def typecheck_partial_eval_jaxpr(jaxpr, unks_in, unks_out, jaxpr1, jaxpr2):\n",
    "  jaxprty = typecheck_jaxpr(jaxpr)    # (a1,  a2) -> (b1, b2 )\n",
    "  jaxpr1ty = typecheck_jaxpr(jaxpr1)  #  a1       -> (b1, res)\n",
    "  jaxpr2ty = typecheck_jaxpr(jaxpr2)  # (res, a2) -> b2\n",
    "\n",
    "  a1, a2 = partition_list(unks_in, jaxprty.in_types)\n",
    "  b1, b2 = partition_list(unks_out, jaxprty.out_types)\n",
    "  b1_, res = split_list(jaxpr1ty.out_types, len(b1))\n",
    "  res_, a2_ = split_list(jaxpr2ty.in_types, len(res))\n",
    "  b2_ = jaxpr2ty.out_types\n",
    "\n",
    "  if jaxpr1ty.in_types != a1: raise TypeError\n",
    "  if jaxpr2ty.out_types != b2: raise TypeError\n",
    "  if b1 != b1_: raise TypeError\n",
    "  if res != res_: raise TypeError\n",
    "  if a2 != a2_: raise TypeError\n",
    "  if b2 != b2_: raise TypeError\n",
    "\n",
    "partial_eval_jaxpr_rules = {}\n",
    "\n",
    "def xla_call_peval_eqn(unks_in: list[bool], eqn: JaxprEqn,\n",
    "                       ) -> tuple[JaxprEqn, JaxprEqn, list[bool], list[Var]]:\n",
    "  jaxpr = eqn.params['jaxpr']\n",
    "  jaxpr1, jaxpr2, unks_out, num_res = partial_eval_jaxpr(jaxpr, unks_in)\n",
    "  ins1, ins2 = partition_list(unks_in, eqn.inputs)\n",
    "  out_binders1, out_binders2 = partition_list(unks_out, eqn.out_binders)\n",
    "  residuals = [Var(v.aval) for v in jaxpr2.in_binders[:num_res]]\n",
    "  eqn1 = JaxprEqn(xla_call_p, ins1, dict(jaxpr=jaxpr1, num_consts=0),\n",
    "                  out_binders1 + residuals)\n",
    "  eqn2 = JaxprEqn(xla_call_p, residuals + ins2,\n",
    "                  dict(jaxpr=jaxpr2, num_consts=0), out_binders2)\n",
    "  return eqn1, eqn2, unks_out, residuals\n",
    "partial_eval_jaxpr_rules[xla_call_p] = xla_call_peval_eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7177599838802657 2.979984993200891\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "  y = sin(x) * 2.\n",
    "  z = - y + x\n",
    "  return z\n",
    "\n",
    "y, f_lin = linearize(f, 3.)\n",
    "y_dot = f_lin(1.)\n",
    "print(y, y_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7077524804807109 -2.121105001260758\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "  y = sin(x) * 2.\n",
    "  z = g(x, y)\n",
    "  return z\n",
    "\n",
    "@jit\n",
    "def g(x, y):\n",
    "  return cos(x) + y\n",
    "\n",
    "y, f_lin = linearize(f, 3.)\n",
    "y_dot = f_lin(1.)\n",
    "print(y, y_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vjpp(f, x):\n",
    "  y, f_lin = linearize(f, x)\n",
    "  f_vjp = lambda y_bar: transpose(f_lin)(y_bar)\n",
    "  return y, f_vjp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PartialVal(aval=ShapedArray(shape=(), dtype=float64), const=1.0), PartialVal(aval=ShapedArray(shape=(), dtype=float64), const=array(1.11766199)), PartialVal(aval=ShapedArray(shape=(), dtype=float64), const=None), PartialVal(aval=ShapedArray(shape=(), dtype=float64), const=None)]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: 3.0*x + 2.0\n",
    "primals_in = (1.0, _)\n",
    "\n",
    "pvals_in = ([PartialVal.known(x) for x in primals_in] + \n",
    "            [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n",
    "\n",
    "print(pvals_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda a:float64[], b:float64[] .\n",
      "  let c:float64[] = exp a\n",
      "      d:float64[] = add c b\n",
      "  in ( d ) }\n",
      "{ lambda a:float64[], b:float64[], c:float64[], d:float64[] .\n",
      "  let e:float64[] = exp a\n",
      "      f:float64[] = exp a\n",
      "      g:float64[] = mul f c\n",
      "      h:float64[] = add e b\n",
      "      i:float64[] = add g d\n",
      "  in ( h, i ) }\n"
     ]
    }
   ],
   "source": [
    "def g(x,y):\n",
    "    return exp(x) + y\n",
    "\n",
    "g_jaxpr, g_consts, _ = make_jaxpr(g, get_aval(1.0), get_aval(1.0))\n",
    "print(g_jaxpr)\n",
    "\n",
    "\n",
    "# partial(jvp, f)\n",
    "\n",
    "def g_jvp(primals, tangents):\n",
    "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
    "    return primals_out, tangents_out\n",
    "\n",
    "# fjvp_jaxpr = make_jaxpr(f_lin, (get_aval(1.0),), (get_aval(1.0),))\n",
    "gjvp_jaxpr, gjvp_consts, _ = make_jaxpr(g_jvp, (get_aval(1.0), get_aval(1.0),), (get_aval(1.0),get_aval(1.0)))\n",
    "print(gjvp_jaxpr)\n",
    "\n",
    "# eval_jaxpr(g_jaxpr, (1.0,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda a:float64[] .\n",
      "  let b:float64[] = mul 3.0 a\n",
      "      c:float64[] = add b 2.0\n",
      "  in ( c ) }\n",
      "{ lambda a:float64[], b:float64[] .\n",
      "  let c:float64[] = mul 3.0 a\n",
      "      d:float64[] = mul 0.0 a\n",
      "      e:float64[] = mul 3.0 b\n",
      "      f:float64[] = add d e\n",
      "      g:float64[] = add c 2.0\n",
      "      h:float64[] = add f 0.0\n",
      "  in ( g, h ) }\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(fjvp_jaxpr)\n\u001b[1;32m     17\u001b[0m eval_jaxpr(f_jaxpr, (\u001b[39m1.0\u001b[39m,))\n\u001b[0;32m---> 19\u001b[0m eval_jaxpr_transposed(f_jaxpr,[ UndefPrimal(\u001b[39m1.0\u001b[39;49m)], [UndefPrimal(\u001b[39m1.0\u001b[39;49m)])\n",
      "Cell \u001b[0;32mIn[160], line 27\u001b[0m, in \u001b[0;36meval_jaxpr_transposed\u001b[0;34m(jaxpr, args, cotangents)\u001b[0m\n\u001b[1;32m     25\u001b[0m   cts_in \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(read_cotangent, eqn\u001b[39m.\u001b[39mout_binders)\n\u001b[1;32m     26\u001b[0m   rule \u001b[39m=\u001b[39m transpose_rules[eqn\u001b[39m.\u001b[39mprimitive]\n\u001b[0;32m---> 27\u001b[0m   cts_out \u001b[39m=\u001b[39m rule(cts_in, \u001b[39m*\u001b[39;49mprimals_in, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meqn\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     28\u001b[0m   \u001b[39mmap\u001b[39m(write_cotangent, eqn\u001b[39m.\u001b[39minputs, cts_out)\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m [read_cotangent(v) \u001b[39mfor\u001b[39;00m v, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(jaxpr\u001b[39m.\u001b[39min_binders, args)\n\u001b[1;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(x) \u001b[39mis\u001b[39;00m UndefPrimal]\n",
      "Cell \u001b[0;32mIn[161], line 4\u001b[0m, in \u001b[0;36mmul_transpose_rule\u001b[0;34m(cts, x, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m z_bar, \u001b[39m=\u001b[39m cts\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m (\u001b[39mtype\u001b[39m(x) \u001b[39mis\u001b[39;00m UndefPrimal) \u001b[39m^\u001b[39m (\u001b[39mtype\u001b[39m(y) \u001b[39mis\u001b[39;00m UndefPrimal)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mreturn\u001b[39;00m [mul(z_bar, y), \u001b[39mNone\u001b[39;00m] \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(x) \u001b[39mis\u001b[39;00m UndefPrimal \u001b[39melse\u001b[39;00m [\u001b[39mNone\u001b[39;00m, mul(x, z_bar)]\n",
      "Cell \u001b[0;32mIn[66], line 21\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x, y)\u001b[0m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(x, y): \u001b[39mreturn\u001b[39;00m bind1(mul_p, x, y)\n",
      "Cell \u001b[0;32mIn[66], line 40\u001b[0m, in \u001b[0;36mbind1\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind1\u001b[39m(prim, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m---> 40\u001b[0m   out, \u001b[39m=\u001b[39m bind(prim, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m     41\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mbind\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(prim, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m      2\u001b[0m   top_trace \u001b[39m=\u001b[39m find_top_trace(args)\n\u001b[0;32m----> 3\u001b[0m   tracers \u001b[39m=\u001b[39m [full_raise(top_trace, arg) \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m args]\n\u001b[1;32m      4\u001b[0m   outs \u001b[39m=\u001b[39m top_trace\u001b[39m.\u001b[39mprocess_primitive(prim, tracers, params)\n\u001b[1;32m      5\u001b[0m   \u001b[39mreturn\u001b[39;00m [full_lower(out) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(prim, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m      2\u001b[0m   top_trace \u001b[39m=\u001b[39m find_top_trace(args)\n\u001b[0;32m----> 3\u001b[0m   tracers \u001b[39m=\u001b[39m [full_raise(top_trace, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m      4\u001b[0m   outs \u001b[39m=\u001b[39m top_trace\u001b[39m.\u001b[39mprocess_primitive(prim, tracers, params)\n\u001b[1;32m      5\u001b[0m   \u001b[39mreturn\u001b[39;00m [full_lower(out) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs]\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mfull_raise\u001b[0;34m(trace, val)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_raise\u001b[39m(trace: Trace, val: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tracer:\n\u001b[1;32m      8\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, Tracer):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(val) \u001b[39min\u001b[39;00m jax_types\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m trace\u001b[39m.\u001b[39mpure(val)\n\u001b[1;32m     11\u001b[0m   level \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39mmain\u001b[39m.\u001b[39mlevel\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3.0 * x + 2.0\n",
    "\n",
    "f_jaxpr, f_consts, _ = make_jaxpr(f, get_aval(1.0))\n",
    "print(f_jaxpr)\n",
    "\n",
    "# partial(jvp, f)\n",
    "\n",
    "def f_jvp(primals, tangents):\n",
    "    primals_out, tangents_out = jvp(f, primals, tangents)\n",
    "    return primals_out, tangents_out\n",
    "\n",
    "# fjvp_jaxpr = make_jaxpr(f_lin, (get_aval(1.0),), (get_aval(1.0),))\n",
    "fjvp_jaxpr, fjvp_consts, _ = make_jaxpr(f_lin, (get_aval(1.0),), (get_aval(1.0),))\n",
    "print(fjvp_jaxpr)\n",
    "\n",
    "eval_jaxpr(f_jaxpr, (1.0,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pejvp_flat(f, *primals_in):\n",
    "  # create known partial vals for primals, and unknwon for tangents\n",
    "  pvals_in = ([PartialVal.known(x) for x  in primals_in] + \n",
    "              [PartialVal.unknown(x) for x in primals_in])\n",
    "  \n",
    "  # jacobean vector product of the function (push_forward)\n",
    "  def f_jvp(*primals_tangents_in):\n",
    "    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n",
    "    return [*primals_out, *tangents_out]\n",
    "  \n",
    "  # linearize the jvp by partial evaluation on the primals\n",
    "  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)\n",
    "\n",
    "  print(\"JAXPR: ------ \")\n",
    "  print(jaxpr)\n",
    "\n",
    "\n",
    "  primal_pvals, _ = split_half(pvals_out)\n",
    "  assert all(pval.is_known for pval in primal_pvals)\n",
    "  primals_out = [pval.const for pval in primal_pvals]\n",
    "\n",
    "  transpose_inputs = consts + [UndefPrimal(p.aval) for p in tangent_pvals_in]\n",
    "  def f_vjp(*cts):\n",
    "    return eval_jaxpr_transposed(jaxpr, transpose_inputs, cts)\n",
    "  \n",
    "  return primals_out, f_vjp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vjp_flat(f, *primals_in):\n",
    "  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n",
    "              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n",
    "  primal_pvals_in, tangent_pvals_in = split_half(pvals_in)\n",
    "  def f_jvp(*primals_tangents_in):\n",
    "    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n",
    "    return [*primals_out, *tangents_out]\n",
    "  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)  # linearize\n",
    "  primal_pvals, _ = split_half(pvals_out)\n",
    "  assert all(pval.is_known for pval in primal_pvals)\n",
    "  primals_out = [pval.const for pval in primal_pvals]\n",
    "  transpose_inputs = consts + [UndefPrimal(p.aval) for p in tangent_pvals_in]\n",
    "  f_vjp = lambda *cts: eval_jaxpr_transposed(jaxpr, transpose_inputs, cts)\n",
    "  return primals_out, f_vjp\n",
    "\n",
    "def vjp(f, *primals_in):\n",
    "  primals_in_flat, in_tree = tree_flatten(primals_in)\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "  primals_out_flat, f_vjp_flat = vjp_flat(f, *primals_in_flat)\n",
    "  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n",
    "\n",
    "  def f_vjp(*cotangents_out):\n",
    "    cotangents_out_flat, _ = tree_flatten(cotangents_out)\n",
    "    cotangents_in_flat = f_vjp_flat(*cotangents_out_flat)\n",
    "    return tree_unflatten(in_tree, cotangents_in_flat)\n",
    "\n",
    "  return primals_out, f_vjp\n",
    "\n",
    "class UndefPrimal(NamedTuple):\n",
    "  aval: ShapedArray\n",
    "\n",
    "register_pytree_node(UndefPrimal,\n",
    "                     lambda u: (u.aval, ()),\n",
    "                     lambda aval, _: UndefPrimal(aval))\n",
    "# NB: the analogous function in JAX is called 'backward_pass'\n",
    "def eval_jaxpr_transposed(jaxpr: Jaxpr, args: list[Any], cotangents: list[Any]\n",
    "                          ) -> list[Any]:\n",
    "  primal_env: dict[Var, Any] = {}\n",
    "  ct_env: dict[Var, Any] = {}\n",
    "\n",
    "  def read_primal(x: Atom) -> Any:\n",
    "    return primal_env.get(x, UndefPrimal(x.aval)) if type(x) is Var else x.val\n",
    "\n",
    "  def write_primal(v: Var, val: Any) -> None:\n",
    "    if type(val) is not UndefPrimal:\n",
    "      primal_env[v] = val\n",
    "\n",
    "  def read_cotangent(v: Var) -> Any:\n",
    "    return ct_env.pop(v, np.zeros(v.aval.shape, v.aval.dtype))\n",
    "\n",
    "  def write_cotangent(x: Atom, val: Any):\n",
    "    if type(x) is Var and val is not None:\n",
    "      ct_env[x] = add(ct_env[x], val) if x in ct_env else val\n",
    "\n",
    "  map(write_primal, jaxpr.in_binders, args)\n",
    "  map(write_cotangent, jaxpr.outs, cotangents)\n",
    "  for eqn in jaxpr.eqns[::-1]:\n",
    "    primals_in = map(read_primal, eqn.inputs)\n",
    "    cts_in = map(read_cotangent, eqn.out_binders)\n",
    "    rule = transpose_rules[eqn.primitive]\n",
    "    cts_out = rule(cts_in, *primals_in, **eqn.params)\n",
    "    map(write_cotangent, eqn.inputs, cts_out)\n",
    "\n",
    "  return [read_cotangent(v) for v, x in zip(jaxpr.in_binders, args)\n",
    "          if type(x) is UndefPrimal]\n",
    "\n",
    "transpose_rules = {}\n",
    "def mul_transpose_rule(cts, x, y):\n",
    "  z_bar, = cts\n",
    "  assert (type(x) is UndefPrimal) ^ (type(y) is UndefPrimal)\n",
    "  return [mul(z_bar, y), None] if type(x) is UndefPrimal else [None, mul(x, z_bar)]\n",
    "transpose_rules[mul_p] = mul_transpose_rule\n",
    "\n",
    "def neg_transpose_rule(cts, x):\n",
    "  ybar, = cts\n",
    "  assert type(x) is UndefPrimal\n",
    "  return [neg(ybar)]\n",
    "transpose_rules[neg_p] = neg_transpose_rule\n",
    "\n",
    "def add_transpose_rule(cts, x, y):\n",
    "  z_bar, = cts\n",
    "  return [z_bar, z_bar]\n",
    "transpose_rules[add_p] = add_transpose_rule\n",
    "\n",
    "def reduce_sum_transpose_rule(cts, x, *, axis):\n",
    "  y_bar, = cts\n",
    "  return [broadcast(y_bar, x.aval.shape, axis)]\n",
    "transpose_rules[reduce_sum_p] = reduce_sum_transpose_rule\n",
    "\n",
    "def xla_call_transpose_rule(cts, *invals, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  undef_primals = [type(x) is UndefPrimal for x in invals]\n",
    "  transposed_jaxpr, new_consts = transpose_jaxpr(jaxpr, tuple(undef_primals))\n",
    "  residuals, _ = partition_list(undef_primals, invals)\n",
    "  outs = bind(xla_call_p, *new_consts, *residuals, *cts,\n",
    "              jaxpr=transposed_jaxpr, num_consts=len(new_consts))\n",
    "  outs = iter(outs)\n",
    "  return [next(outs) if undef else None for undef in undef_primals]\n",
    "transpose_rules[xla_call_p] = xla_call_transpose_rule\n",
    "\n",
    "@lru_cache()\n",
    "def transpose_jaxpr(jaxpr: Jaxpr, undef_primals: tuple[bool, ...]\n",
    "                    ) -> tuple[Jaxpr, list[Any]]:\n",
    "  avals_in, avals_out = typecheck_jaxpr(jaxpr)\n",
    "  traceable = partial(eval_jaxpr_transposed, jaxpr)\n",
    "  args = [UndefPrimal(a) if u else a for a, u in zip(avals_in, undef_primals)]\n",
    "  trans_jaxpr, consts, _ = make_jaxpr(traceable, tuple(args), tuple(avals_out))\n",
    "  typecheck_jaxpr(trans_jaxpr)\n",
    "  return trans_jaxpr, consts\n",
    "def grad(f):\n",
    "  def gradfun(x, *xs):\n",
    "    y, f_vjp = vjp(f, x, *xs)\n",
    "    if np.shape(y) != (): raise TypeError\n",
    "    x_bar, *_ = f_vjp(np.ones(np.shape(y), np.result_type(y)))\n",
    "    return x_bar\n",
    "  return gradfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.9899924966004454,) -0.9899924966004454\n"
     ]
    }
   ],
   "source": [
    "y, f_vjp = vjp(sin, 3.)\n",
    "print(f_vjp(1.), cos(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.979984993200891\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  y = sin(x) * 2.\n",
    "  z = - y + x\n",
    "  return z\n",
    "\n",
    "print(grad(f)(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1176619927957034\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "  y = x * 2.\n",
    "  z = g(y)\n",
    "  return z\n",
    "\n",
    "@jit\n",
    "def g(x):\n",
    "  return cos(x) * 2.\n",
    "\n",
    "print(grad(f)(3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core_test.py fun_with_nested_calls_2\n",
    "def foo(x):\n",
    "  @jit\n",
    "  def bar(y):\n",
    "    def baz(w):\n",
    "      q = jit(lambda x: y)(x)\n",
    "      q = q + jit(lambda: y)()\n",
    "      q = q + jit(lambda y: w + y)(y)\n",
    "      q = jit(lambda w: jit(sin)(x) * y)(1.0) + q\n",
    "      return q\n",
    "    p, t = jvp(baz, (x + 1.0,), (y,))\n",
    "    return t + (x * p)\n",
    "  return bar(x)\n",
    "\n",
    "def assert_allclose(*vals):\n",
    "  for v1, v2 in zip(vals[:-1], vals[1:]):\n",
    "    np.testing.assert_allclose(v1, v2)\n",
    "\n",
    "ans1 = f(3.)\n",
    "ans2 = jit(f)(3.)\n",
    "ans3, _ = jvp(f, (3.,), (5.,))\n",
    "ans4, _ = jvp(jit(f), (3.,), (5.,))\n",
    "assert_allclose(ans1, ans2, ans3, ans4)\n",
    "\n",
    "deriv1 = grad(f)(3.)\n",
    "deriv2 = grad(jit(f))(3.)\n",
    "deriv3 = jit(grad(jit(f)))(3.)\n",
    "_, deriv4 = jvp(f, (3.,), (1.,))\n",
    "_, deriv5 = jvp(jit(f), (3.,), (1.,))\n",
    "assert_allclose(deriv1, deriv2, deriv3, deriv4, deriv5)\n",
    "\n",
    "hess1 = grad(grad(f))(3.)\n",
    "hess2 = grad(grad(jit(f)))(3.)\n",
    "hess3 = grad(jit(grad(f)))(3.)\n",
    "hess4 = jit(grad(grad(f)))(3.)\n",
    "_, hess5 = jvp(grad(f), (3.,), (1.,))\n",
    "_, hess6 = jvp(jit(grad(f)), (3.,), (1.,))\n",
    "_, hess7 = jvp(jit(grad(f)), (3.,), (1.,))\n",
    "assert_allclose(hess1, hess2, hess3, hess4, hess5, hess6, hess7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.vjp.<locals>.f_vjp(*cotangents_out)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def h(x):\n",
    "    return 2.0*x + 2.0\n",
    "\n",
    "y, f_vjp = vjp(h, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vjp_flat(f, *primals_in):\n",
    "  pvals_in = ([PartialVal.known(x) for x in primals_in] +\n",
    "              [PartialVal.unknown(vspace(get_aval(x))) for x in primals_in])\n",
    "  primal_pvals_in, tangent_pvals_in = split_half(pvals_in)\n",
    "  def f_jvp(*primals_tangents_in):\n",
    "    primals_out, tangents_out = jvp(f, *split_half(primals_tangents_in))\n",
    "    return [*primals_out, *tangents_out]\n",
    "  jaxpr, pvals_out, consts = partial_eval_flat(f_jvp, pvals_in)  # linearize\n",
    "  primal_pvals, _ = split_half(pvals_out)\n",
    "  assert all(pval.is_known for pval in primal_pvals)\n",
    "  primals_out = [pval.const for pval in primal_pvals]\n",
    "  transpose_inputs = consts + [UndefPrimal(p.aval) for p in tangent_pvals_in]\n",
    "  f_vjp = lambda *cts: eval_jaxpr_transposed(jaxpr, transpose_inputs, cts)\n",
    "  return primals_out, f_vjp\n",
    "\n",
    "def vjp(f, *primals_in):\n",
    "  primals_in_flat, in_tree = tree_flatten(primals_in)\n",
    "  f, out_tree = flatten_fun(f, in_tree)\n",
    "  primals_out_flat, f_vjp_flat = vjp_flat(f, *primals_in_flat)\n",
    "  primals_out = tree_unflatten(out_tree(), primals_out_flat)\n",
    "\n",
    "  def f_vjp(*cotangents_out):\n",
    "    cotangents_out_flat, _ = tree_flatten(cotangents_out)\n",
    "    cotangents_in_flat = f_vjp_flat(*cotangents_out_flat)\n",
    "    return tree_unflatten(in_tree, cotangents_in_flat)\n",
    "\n",
    "  return primals_out, f_vjp\n",
    "\n",
    "class UndefPrimal(NamedTuple):\n",
    "  aval: ShapedArray\n",
    "\n",
    "register_pytree_node(UndefPrimal,\n",
    "                     lambda u: (u.aval, ()),\n",
    "                     lambda aval, _: UndefPrimal(aval))\n",
    "# NB: the analogous function in JAX is called 'backward_pass'\n",
    "def eval_jaxpr_transposed(jaxpr: Jaxpr, args: list[Any], cotangents: list[Any]\n",
    "                          ) -> list[Any]:\n",
    "  primal_env: dict[Var, Any] = {}\n",
    "  ct_env: dict[Var, Any] = {}\n",
    "\n",
    "  def read_primal(x: Atom) -> Any:\n",
    "    return primal_env.get(x, UndefPrimal(x.aval)) if type(x) is Var else x.val\n",
    "\n",
    "  def write_primal(v: Var, val: Any) -> None:\n",
    "    if type(val) is not UndefPrimal:\n",
    "      primal_env[v] = val\n",
    "\n",
    "  def read_cotangent(v: Var) -> Any:\n",
    "    return ct_env.pop(v, np.zeros(v.aval.shape, v.aval.dtype))\n",
    "\n",
    "  def write_cotangent(x: Atom, val: Any):\n",
    "    if type(x) is Var and val is not None:\n",
    "      ct_env[x] = add(ct_env[x], val) if x in ct_env else val\n",
    "\n",
    "  map(write_primal, jaxpr.in_binders, args)\n",
    "  map(write_cotangent, jaxpr.outs, cotangents)\n",
    "  for eqn in jaxpr.eqns[::-1]:\n",
    "    primals_in = map(read_primal, eqn.inputs)\n",
    "    cts_in = map(read_cotangent, eqn.out_binders)\n",
    "    rule = transpose_rules[eqn.primitive]\n",
    "    cts_out = rule(cts_in, *primals_in, **eqn.params)\n",
    "    map(write_cotangent, eqn.inputs, cts_out)\n",
    "\n",
    "  return [read_cotangent(v) for v, x in zip(jaxpr.in_binders, args)\n",
    "          if type(x) is UndefPrimal]\n",
    "\n",
    "transpose_rules = {}\n",
    "def mul_transpose_rule(cts, x, y):\n",
    "  z_bar, = cts\n",
    "  assert (type(x) is UndefPrimal) ^ (type(y) is UndefPrimal)\n",
    "  return [mul(z_bar, y), None] if type(x) is UndefPrimal else [None, mul(x, z_bar)]\n",
    "transpose_rules[mul_p] = mul_transpose_rule\n",
    "\n",
    "def neg_transpose_rule(cts, x):\n",
    "  ybar, = cts\n",
    "  assert type(x) is UndefPrimal\n",
    "  return [neg(ybar)]\n",
    "transpose_rules[neg_p] = neg_transpose_rule\n",
    "\n",
    "def add_transpose_rule(cts, x, y):\n",
    "  z_bar, = cts\n",
    "  return [z_bar, z_bar]\n",
    "transpose_rules[add_p] = add_transpose_rule\n",
    "\n",
    "def reduce_sum_transpose_rule(cts, x, *, axis):\n",
    "  y_bar, = cts\n",
    "  return [broadcast(y_bar, x.aval.shape, axis)]\n",
    "transpose_rules[reduce_sum_p] = reduce_sum_transpose_rule\n",
    "\n",
    "def xla_call_transpose_rule(cts, *invals, jaxpr, num_consts):\n",
    "  del num_consts  # Unused\n",
    "  undef_primals = [type(x) is UndefPrimal for x in invals]\n",
    "  transposed_jaxpr, new_consts = transpose_jaxpr(jaxpr, tuple(undef_primals))\n",
    "  residuals, _ = partition_list(undef_primals, invals)\n",
    "  outs = bind(xla_call_p, *new_consts, *residuals, *cts,\n",
    "              jaxpr=transposed_jaxpr, num_consts=len(new_consts))\n",
    "  outs = iter(outs)\n",
    "  return [next(outs) if undef else None for undef in undef_primals]\n",
    "transpose_rules[xla_call_p] = xla_call_transpose_rule\n",
    "\n",
    "@lru_cache()\n",
    "def transpose_jaxpr(jaxpr: Jaxpr, undef_primals: tuple[bool, ...]\n",
    "                    ) -> tuple[Jaxpr, list[Any]]:\n",
    "  avals_in, avals_out = typecheck_jaxpr(jaxpr)\n",
    "  traceable = partial(eval_jaxpr_transposed, jaxpr)\n",
    "  args = [UndefPrimal(a) if u else a for a, u in zip(avals_in, undef_primals)]\n",
    "  trans_jaxpr, consts, _ = make_jaxpr(traceable, tuple(args), tuple(avals_out))\n",
    "  typecheck_jaxpr(trans_jaxpr)\n",
    "  return trans_jaxpr, consts\n",
    "def grad(f):\n",
    "  def gradfun(x, *xs):\n",
    "    y, f_vjp = vjp(f, x, *xs)\n",
    "    if np.shape(y) != (): raise TypeError\n",
    "    x_bar, *_ = f_vjp(np.ones(np.shape(y), np.result_type(y)))\n",
    "    return x_bar\n",
    "  return gradfun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
